{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRosero/aml-project/blob/master/train_step_4_FDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9D12pC0R_eq"
      },
      "source": [
        "# Importing the dataset from drive.\n",
        "( You can find the zipped folder [here](https://drive.google.com/file/d/1XsRmyQYHfgRFJCOueXpJ37yyOCrKHO-W/view?usp=sharing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOzT4IA9ZnnX",
        "outputId": "30d687d6-3114-462f-df97-19cf6b54a1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive/\n",
            "download cropped dataset\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gPcwDJsNpyqcjKu225hnIxkURpjUA08i\n",
            "To: /content/data.zip\n",
            "100% 73.4M/73.4M [00:01<00:00, 62.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive/')\n",
        "data_path = \"/content/data\"\n",
        "\n",
        "use_complete_dataset = False\n",
        "\n",
        "\n",
        "if (use_complete_dataset == True) and (not os.path.isfile('/content/data.zip')):\n",
        "  print(\"download entire dataset\")\n",
        "  !gdown --id 1A2dBwPlCyXHTqmG1LRvPfVm6K21jWwUI # 3-5 min\n",
        "  !jar xf  \"/content/data.zip\"\n",
        "elif (use_complete_dataset == False) and (not os.path.isfile('/content/data.zip')):\n",
        "  # Load cropped dataset containing only 10 images\n",
        "  print(\"download cropped dataset\")\n",
        "  !gdown --id 1gPcwDJsNpyqcjKu225hnIxkURpjUA08i   \n",
        "  !jar xf  \"/content/data.zip\"\n",
        "else:\n",
        "  print(\"dataset already downloaded\")\n",
        "\n",
        "if not os.path.isdir('/content/data'):\n",
        "  print(\"Dataset doesn't exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1nqqrU5SIBH"
      },
      "source": [
        "# Cloning the repository from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uzIXW9Z5Kl4",
        "outputId": "6d3022b2-0ece-4f83-fe83-eed20cac95c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (551/551), done.\u001b[K\n",
            "remote: Compressing objects: 100% (514/514), done.\u001b[K\n",
            "remote: Total 600 (delta 265), reused 138 (delta 34), pack-reused 49\u001b[K\n",
            "Receiving objects: 100% (600/600), 25.08 MiB | 2.05 MiB/s, done.\n",
            "Resolving deltas: 100% (287/287), done.\n",
            "/content/cloned-repo\n",
            "/content/cloned-repo\n",
            "dataset      image_output  results  train_step_2.ipynb\t    train_step_4.ipynb\n",
            "demo_images  model\t   runs     train_step_3.ipynb\t    utils\n",
            "FDA.ipynb    README.md\t   scripts  train_step_4_FDA.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "repo_path = \"/content/cloned-repo\"\n",
        "if not os.path.isdir(repo_path):\n",
        "  !git clone -l -s https://github.com/CRosero/aml-project.git cloned-repo\n",
        "  %cd cloned-repo\n",
        "else:\n",
        "  print(\"Repository already cloned\")\n",
        "%cd /content/cloned-repo\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4np6uwUgDMW"
      },
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06ImxSTmbCnz",
        "outputId": "d92dc098-ccaf-4181-966b-df5d011addee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QYHTsZf6SkxK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils import data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from utils.utils import reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu, colour_code_segmentation,poly_lr_scheduler\n",
        "from utils.loss import CrossEntropy2d,DiceLoss\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path as osp\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from PIL import Image\n",
        "from torchinfo import summary\n",
        "import json\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Dataset class:\n",
        "from dataset.cityscapesDataSet import cityscapesDataSet\n",
        "from dataset.GTA5DataSet import GTA5DataSet\n",
        "# Discriminator\n",
        "from model.discriminator import FCDiscriminator, LightWeightFCDiscriminator\n",
        "# Network\n",
        "from model.build_BiSeNet import BiSeNet\n",
        "# Validation function\n",
        "from scripts.eval import val\n",
        "# FDA\n",
        "from utils.FDA import FDA_source_to_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9TWPNCay2Md",
        "outputId": "287d00c5-b8ec-46c8-efc7-fba3cd87b8ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "evfJ9HAhM9-a"
      },
      "outputs": [],
      "source": [
        "def enable_cuda(obj, gpu):\n",
        "  if torch.cuda.is_available():\n",
        "    return obj.cuda(gpu)\n",
        "  else:\n",
        "    return obj\n",
        "    \n",
        "def loss_calc(pred, labels, gpu, ignore_label=255):\n",
        "    \"\"\"\n",
        "    This function returns cross entropy loss for semantic segmentation\n",
        "    \"\"\"\n",
        "    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n",
        "    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n",
        "    labels = Variable(labels.long()).cuda(gpu)\n",
        "    labels = enable_cuda(labels, gpu)\n",
        "    criterion = CrossEntropy2d(ignore_label= ignore_label)\n",
        "    criterion = enable_cuda(criterion, gpu)\n",
        "\n",
        "    return criterion(pred, labels)\n",
        "\n",
        "\n",
        "def lr_poly(base_lr, iter, max_iter, power):\n",
        "    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n",
        "\n",
        "\n",
        "def adjust_learning_rate(args, optimizer, i_iter):\n",
        "    lr = lr_poly(args.learning_rate, i_iter, args.num_steps, args.power)\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    if len(optimizer.param_groups) > 1:\n",
        "        optimizer.param_groups[1]['lr'] = lr * 10\n",
        "\n",
        "\n",
        "def adjust_learning_rate_D(args, optimizer, i_iter):\n",
        "    lr = lr_poly(args.learning_rate_D, i_iter, args.num_steps, args.power)\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    if len(optimizer.param_groups) > 1:\n",
        "        optimizer.param_groups[1]['lr'] = lr * 10\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, model, model_D, optimizer,optimizer_D, sourceloader, targetloader, targetloaderVal, iter_size, miou_init=0, iter_start_i=0):\n",
        "  # labels for adversarial training\n",
        "  source_label_id = 0\n",
        "  target_label_id = 1\n",
        "\n",
        "  if args.gan == 'Vanilla':\n",
        "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
        "  elif args.gan == 'LS':\n",
        "    bce_loss = torch.nn.MSELoss()\n",
        "\n",
        "  writer = SummaryWriter()\n",
        "  max_miou = miou_init\n",
        "  \n",
        "  if args.FDA:\n",
        "    IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "    IMG_MEAN = torch.reshape( torch.from_numpy(IMG_MEAN), (1,3,1,1)  )\n",
        "    mean_img = torch.zeros(1, 1)\n",
        "\n",
        "  for i_iter in range(iter_start_i, args.num_steps):\n",
        "\n",
        "    loss_seg_value = 0\n",
        "    loss_adv_target_value = 0\n",
        "    loss_D_value = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    adjust_learning_rate(args, optimizer, i_iter)\n",
        "\n",
        "    optimizer_D.zero_grad()\n",
        "    adjust_learning_rate_D(args, optimizer_D, i_iter)\n",
        "    sourceloader_iter = enumerate(sourceloader)\n",
        "    targetloader_iter = enumerate(targetloader)\n",
        "    \n",
        "    for sub_i in range(iter_size): \n",
        "        # train G\n",
        "        model.train()\n",
        "\n",
        "        \n",
        "        # don't accumulate grads in D\n",
        "        for param in model_D.parameters():\n",
        "            param.requires_grad = False\n",
        "        # get batch from dataloaders\n",
        "        _, batch_source = next(sourceloader_iter)  # new batch source\n",
        "        source_images, source_labels = batch_source\n",
        "        _, batch_target = next(targetloader_iter) # new batch target\n",
        "        target_images, _ = batch_target\n",
        "\n",
        "        if args.FDA: \n",
        "          if mean_img.shape[-1] < 2:\n",
        "              B, C, H, W = source_images.shape\n",
        "              mean_img = IMG_MEAN.repeat(B,1,H,W)\n",
        "          #-------------------------------------------------------------------#\n",
        "          # 1. source to target, target to target\n",
        "          src_in_trg = FDA_source_to_target( source_images, target_images, L=args.LB )            # src_lbl\n",
        "          trg_in_trg = target_images\n",
        "\n",
        "          # 2. subtract mean\n",
        "          source_images = src_in_trg.clone() - mean_img   # src, src_lbl\n",
        "          target_images = trg_in_trg.clone() - mean_img   # trg, trg_lbl\n",
        "\n",
        "          #-------------------------------------------------------------------#\n",
        "        \n",
        "        # train with source images and labels\n",
        "        source_images = Variable(source_images)\n",
        "        source_images = enable_cuda(source_images, args.gpu)\n",
        "        source_labels = Variable(source_labels)\n",
        "        source_labels = enable_cuda(source_labels, args.gpu)\n",
        "\n",
        "        pred_source_result, pred_source_1, pred_source_2 = model(source_images)\n",
        "        loss1 = loss_calc(pred_source_result, source_labels, args.gpu, args.ignore_label)\n",
        "        loss2 = loss_calc(pred_source_1, source_labels, args.gpu, args.ignore_label)\n",
        "        loss3 = loss_calc(pred_source_2, source_labels, args.gpu, args.ignore_label)\n",
        "        loss_seg = loss1 + loss2 + loss3\n",
        "\n",
        "        # proper normalization\n",
        "        loss_seg = loss_seg / iter_size\n",
        "        loss_seg.backward()\n",
        "        #print(\"Segmentation loss:\", loss_seg.data.cpu().numpy())\n",
        "        #print(loss_seg.data.cpu().numpy().shape)\n",
        "        loss_seg_value += loss_seg.data.cpu().numpy() / iter_size\n",
        "\n",
        "        # train with target images\n",
        "        target_images = Variable(target_images).cuda(args.gpu)\n",
        "\n",
        "        pred_target_result, pred_target_1, pred_target_2 = model(target_images)\n",
        "\n",
        "        # generator vs. discriminator \n",
        "        D_out = model_D(F.softmax(pred_target_result))\n",
        "\n",
        "        loss_adv_target = bce_loss(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label_id)).cuda(args.gpu))\n",
        "        loss = args.lambda_adv_target * loss_adv_target\n",
        "        loss = loss / iter_size\n",
        "        loss.backward()\n",
        "        loss_adv_target_value += loss_adv_target.data.cpu().numpy() / iter_size\n",
        "\n",
        "        # train discriminator\n",
        "        # bring back requires_grad\n",
        "        for param in model_D.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # train with source\n",
        "        pred_source_result = pred_source_result.detach()\n",
        "        \n",
        "        D_out_source = model_D(F.softmax(pred_source_result))\n",
        "        loss_D_source = bce_loss(D_out_source, Variable(torch.FloatTensor(D_out_source.data.size()).fill_(source_label_id)).cuda(args.gpu))\n",
        "        loss_D_source = loss_D_source / iter_size / 2\n",
        "        loss_D_source.backward()\n",
        "\n",
        "        loss_D_value += loss_D_source.data.cpu().numpy()\n",
        "\n",
        "        # train with target\n",
        "        pred_target_result = pred_target_result.detach()\n",
        "\n",
        "        D_out_target = model_D(F.softmax(pred_target_result))\n",
        "\n",
        "        loss_D_target = bce_loss(D_out_target, Variable(torch.FloatTensor(D_out_target.data.size()).fill_(target_label_id)).cuda(args.gpu))\n",
        "        loss_D_target = loss_D_target / iter_size / 2\n",
        "        loss_D_target.backward()\n",
        "\n",
        "        loss_D_value += loss_D_target.data.cpu().numpy()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    writer.add_scalar('loss_seg_value', loss_seg_value, i_iter)\n",
        "    writer.add_scalar('loss_adv_target_value', loss_adv_target_value, i_iter)\n",
        "    writer.add_scalar('loss_D_value', loss_D_value, i_iter)\n",
        "    print('iter = {0:8d}/{1:8d}, loss_seg = {2:.3f}, loss_adv = {3:.3f}, loss_D = {4:.3f}'.format(i_iter, args.num_steps, loss_seg_value, loss_adv_target_value, loss_D_value))\n",
        "\n",
        "\n",
        "    if i_iter % args.save_pred_every == 0 and i_iter != 0:\n",
        "        print(\" Saving checkpoint in \", args.save_models_path, \"latest_CE_loss.pth\")\n",
        "        if not os.path.isdir(args.save_models_path):\n",
        "            os.mkdir(args.save_models_path)\n",
        "        torch.save({\n",
        "                  'iter': i_iter,\n",
        "                  'segNet_state_dict': model.state_dict(),\n",
        "                  'D_state_dict': model_D.state_dict(),\n",
        "                  'optimizer_seg_state_dict': optimizer.state_dict(),\n",
        "                  'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "                  'max_miou' : max_miou,\n",
        "                    },\n",
        "                    os.path.join(args.save_models_path, 'latest_CE_loss.pth'))\n",
        "\n",
        "    \n",
        "    if i_iter % args.validation_step == 0:\n",
        "        print(\" doing validation at iter \", i_iter)\n",
        "        precision, miou = val(args, model, targetloaderVal)\n",
        "        if miou > max_miou:\n",
        "            max_miou = miou\n",
        "            print(\" Saving checkpoint in \", args.save_models_path, \"best_CE_loss.pth\")\n",
        "            os.makedirs(args.save_models_path, exist_ok=True)\n",
        "            torch.save({\n",
        "                  'iter': i_iter,\n",
        "                  'segNet_state_dict': model.state_dict(),\n",
        "                  'D_state_dict': model_D.state_dict(),\n",
        "                  'optimizer_seg_state_dict': optimizer.state_dict(),\n",
        "                  'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "                  'max_miou' : max_miou,\n",
        "                    },\n",
        "                    os.path.join(args.save_models_path, 'best_CE_loss.pth'))\n",
        "        writer.add_scalar('precision', precision, i_iter)\n",
        "        writer.add_scalar('miou', miou, i_iter)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "xdrFawWmP8qE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S24rDhec9N0b"
      },
      "outputs": [],
      "source": [
        "def get_arguments(params=[]):\n",
        "    \"\"\"Parse all the arguments provided from the CLI.\n",
        "    Returns:\n",
        "      A list of parsed arguments.\n",
        "    \"\"\"\n",
        "        \n",
        "    # basic parameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default='BiseNet',\n",
        "                        help=\"available options : BiseNet\")\n",
        "    parser.add_argument(\"--target\", type=str, default='Cityscapes',\n",
        "                        help=\"available options : Cityscapes\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=2,\n",
        "                        help=\"Number of images sent to the network in one step.\")\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=4,\n",
        "                        help=\"number of workers for multithread dataloading.\")\n",
        "    parser.add_argument(\"--data-dir\", type=str, default='',\n",
        "                        help=\"Path to the directory containing the source dataset.\")\n",
        "    parser.add_argument(\"--ignore-label\", type=int, default= 255,\n",
        "                        help=\"The index of the label to ignore during the training.\")\n",
        "    parser.add_argument(\"--input-size\", type=str, default='1024,512',\n",
        "                        help=\"Comma-separated string with height and width of source images.\")\n",
        "    parser.add_argument(\"--input-size-target\", type=str, default='1024,512',\n",
        "                        help=\"Comma-separated string with height and width of target images.\")\n",
        "    parser.add_argument(\"--is-training\", action=\"store_true\",\n",
        "                        help=\"Whether to updates the running means and variances during the training.\")\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-2,\n",
        "                        help=\"Base learning rate for training with polynomial decay.\")\n",
        "    parser.add_argument(\"--learning-rate-D\", type=float, default=1e-4,\n",
        "                        help=\"Base learning rate for discriminator.\")\n",
        "    parser.add_argument(\"--lambda-seg\", type=float, default=1,\n",
        "                        help=\"lambda_seg.\")\n",
        "    parser.add_argument(\"--lambda-adv-target\", type=float, default=0.001,\n",
        "                        help=\"lambda_adv for adversarial training.\")\n",
        "    parser.add_argument(\"--momentum\", type=float, default=0.9,\n",
        "                        help=\"Momentum component of the optimiser.\")\n",
        "    parser.add_argument(\"--num-classes\", type=int, default=19,\n",
        "                        help=\"Number of classes to predict (including background).\")\n",
        "    parser.add_argument(\"--num-steps\", type=int, default=50,\n",
        "                        help=\"Number of training steps.\")\n",
        "    parser.add_argument(\"--num-steps-stop\", type=int, default=150,\n",
        "                        help=\"Number of training steps for early stopping.\")\n",
        "    parser.add_argument(\"--power\", type=float, default=0.9,\n",
        "                        help=\"Decay parameter to compute the learning rate.\")\n",
        "    parser.add_argument(\"--random-mirror\", action=\"store_true\",\n",
        "                        help=\"Whether to randomly mirror the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-scale\", action=\"store_true\",\n",
        "                        help=\"Whether to randomly scale the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-seed\", type=int, default=42,\n",
        "                        help=\"Random seed to have reproducible results.\")\n",
        "    parser.add_argument(\"--save-pred-every\", type=int, default=10,\n",
        "                        help=\"Save summaries and checkpoint every often.\")\n",
        "    parser.add_argument(\"--weight-decay\", type=float, default=1e-4,\n",
        "                        help=\"Regularisation parameter for L2-loss.\")\n",
        "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
        "                        help=\"choose gpu device.\")\n",
        "    parser.add_argument(\"--gan\", type=str, default='Vanilla',\n",
        "                        help=\"choose the GAN objective.\")\n",
        "    parser.add_argument('--context_path', type=str, default='resnet18',\n",
        "                        help='The context path model you are using, resnet18, resnet101.')\n",
        "    parser.add_argument('--validation_step', type=int, default=10, help='How often to perform validation (epochs)')\n",
        "    parser.add_argument('--use_gpu', type=bool, default=True, help='whether to user gpu for training')\n",
        "    parser.add_argument(\"--light_discriminator\", type=bool, default=False, \n",
        "                        help=\"using discriminator with lightweight depthwise-separable convolutions\")\n",
        "    parser.add_argument('--load_pretrained_models', type=bool, default=False, help='load or not the pretrained models from the saved checkpoint ')\n",
        "    parser.add_argument('--pretrained_models_path', type=str, default=\"\", help='path to pretrained models')\n",
        "    parser.add_argument('--save_models_path', type=str, default=\"\", help='path to save models')\n",
        "    \n",
        "    parser.add_argument('--FDA', type=bool, default=True, help='whether to use FDA to transform source images')\n",
        "    parser.add_argument(\"--LB\", type=float, default=0.1, help=\"beta for FDA\")\n",
        "\n",
        "\n",
        "\n",
        "    args = parser.parse_args(params)\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(params):\n",
        "  \"\"\"Create the model and start the training.\"\"\"\n",
        "  args = get_arguments(params)\n",
        "\n",
        "  # Set random seed\n",
        "  torch.manual_seed(args.random_seed)\n",
        "  torch.cuda.manual_seed(args.random_seed)\n",
        "  np.random.seed(args.random_seed)\n",
        "  random.seed(args.random_seed)\n",
        "\n",
        "  # input sizes\n",
        "  w, h = map(int, args.input_size.split(','))\n",
        "  input_size = (w, h)\n",
        "\n",
        "  w, h = map(int, args.input_size_target.split(','))\n",
        "  input_size_target = (w, h)\n",
        "\n",
        "  cudnn.benchmark = True\n",
        "  cudnn.enabled = True\n",
        "  gpu = args.gpu\n",
        "\n",
        "  # Create network\n",
        "  if args.model == 'BiseNet':\n",
        "    model = BiSeNet(num_classes=args.num_classes, context_path= args.context_path)\n",
        "\n",
        "  # Set the network to train mode\n",
        "  model.train()\n",
        "  model = enable_cuda(model, args.gpu)\n",
        "\n",
        "  # init D\n",
        "  if args.light_discriminator == False:\n",
        "    print(\"Using a fully convolutional discriminator\")\n",
        "    model_D = FCDiscriminator(num_classes=args.num_classes)\n",
        "  else:\n",
        "    print(\"Using a discriminator with lightweight depthwise-separable convolution\")\n",
        "    model_D = LightWeightFCDiscriminator(num_classes=args.num_classes)\n",
        "\n",
        "  # Set the discriminators to train mode\n",
        "  model_D.train()\n",
        "  model_D = enable_cuda(model_D, args.gpu)\n",
        "\n",
        "  \n",
        "  ''' \n",
        "  # Printing statistics\n",
        "  print(\"Segmentation Network\\n\")\n",
        "  print(summary(enable_cuda(model.eval(), args.gpu), input_size=(BATCH_SIZE, 3, input_size[0], input_size[1])))\n",
        "\n",
        "  if args.light_discriminator == False: \n",
        "    print(\"Adversarial discriminator Architecture\\n\")\n",
        "  else:\n",
        "    print(\"Lightweight Adversarial Domain Adaptation\\n\")\n",
        "  print(summary(enable_cuda(model_D.eval(), args.gpu), input_size=(BATCH_SIZE, 19, input_size[0], input_size[1])))\n",
        "  '''\n",
        "  # Path\n",
        "  source_data_root_path = os.path.join(args.data_dir, \"GTA5\") # /content/data/GTA5\n",
        "  target_data_root_path = os.path.join(args.data_dir, args.target) # /content/data/Cityscapes\n",
        "  source_train_path = os.path.join(source_data_root_path, \"train.txt\") # /content/data/GTA5/train.txt\n",
        "  target_root_path = os.path.join(target_data_root_path,  \"train.txt\")   # /content/data/Cityscapes/train.txt\n",
        "  info_path = os.path.join(source_data_root_path,  \"info.json\") # /content/data/GTA/info.json \n",
        "\n",
        "  info_json = json.load(open(info_path))\n",
        "\n",
        "  # Image mean\n",
        "  IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "  # Zero mean\n",
        "  IMG_MEAN_ZERO = np.array((0.0, 0.0, 0.0), dtype=np.float32)\n",
        "\n",
        "  if (args.FDA):\n",
        "    img_mean = IMG_MEAN_ZERO # From FDA original code: \"use the original images for FDA, then do mean subtraction, normalization, etc. Otherwise, will be numerical artifact\"\n",
        "    IMG_MEAN = torch.reshape( torch.from_numpy(IMG_MEAN), (1,3,1,1)  )\n",
        "  else:\n",
        "    img_mean = IMG_MEAN\n",
        "\n",
        "  # Datasets  \n",
        "  source_dataset = GTA5DataSet(source_data_root_path, source_train_path, info_json, crop_size=input_size, scale=args.random_scale, mirror=args.random_mirror, mean=img_mean)\n",
        "  target_dataset = cityscapesDataSet(target_data_root_path, target_root_path, info_json, crop_size=input_size_target, scale=False, mirror=args.random_mirror, mean=img_mean)\n",
        "\n",
        "  print(\"GTA: \", len(source_dataset))\n",
        "  print(\"Cityscapes: \", len(source_dataset))\n",
        "  img,label = source_dataset[0]\n",
        "  print (\"GTA image\", img.shape )\n",
        "  print (\"GTA label\", label.shape )\n",
        "  img, _ = target_dataset[0]\n",
        "  print (\"Cityscapes image\", img.shape )\n",
        "\n",
        "  # Itersize\n",
        "  assert len(source_dataset)==len(target_dataset)\n",
        "  iter_size = len(source_dataset) // args.batch_size # the source and the target have the same len\n",
        "  print(\"Iter_Size = \", iter_size)\n",
        "\n",
        "  # Create DataLoaders\n",
        "  sourceloader = data.DataLoader(source_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "  targetloader = data.DataLoader(target_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "  targetloaderVal = data.DataLoader(target_dataset, batch_size=1, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "  # Optimizer\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  optimizer_D = optim.Adam(model_D.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n",
        "  optimizer_D.zero_grad()\n",
        "\n",
        "  # to keep track of best miou\n",
        "  max_miou = 0\n",
        "  # start iteration from:\n",
        "  iter_start_i = 0\n",
        "\n",
        "  # load pretrained model if exists\n",
        "  checkpoint = None\n",
        "  if (args.load_pretrained_models) and (args.pretrained_models_path is not None) and (os.path.isfile(args.pretrained_models_path)):\n",
        "      print('load models from %s ...' % args.pretrained_models_path)\n",
        "      checkpoint= torch.load(args.pretrained_models_path)\n",
        "      model.load_state_dict(checkpoint['segNet_state_dict'])\n",
        "      model_D.load_state_dict(checkpoint['D_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_seg_state_dict'])\n",
        "      optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "      iter_start_i = int(checkpoint['iter'])+1\n",
        "      max_miou = float(checkpoint['max_miou']) \n",
        "      print('Done! Loaded model trained until iter:', iter_start_i, \"best miou so far:\", max_miou)\n",
        "\n",
        "  # train\n",
        "  train(args, model, model_D, optimizer,optimizer_D, sourceloader, targetloader, targetloaderVal,iter_size, miou_init=max_miou, iter_start_i=iter_start_i)\n",
        "  # final val\n",
        "  val(args, model, targetloaderVal, save=True)"
      ],
      "metadata": {
        "id": "AH9nIjoyN4fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    params = [\n",
        "              '--model', 'BiseNet',\n",
        "              '--target', 'Cityscapes',\n",
        "              '--batch-size', '2',\n",
        "              '--num-workers', '4',\n",
        "              '--data-dir', '/content/data/',\n",
        "              '--ignore-label', '255',\n",
        "              '--input-size', '1024,512',\n",
        "              '--input-size-target',  '1024,512',\n",
        "              '--learning-rate', '2.5e-2',\n",
        "              '--learning-rate-D', '1e-4',\n",
        "              '--lambda-seg', '1',\n",
        "              '--lambda-adv-target', '0.001',\n",
        "              '--momentum', '0.9',\n",
        "              '--power', '0.9',\n",
        "              '--weight-decay','1e-4',\n",
        "              '--num-classes', '19',\n",
        "              '--num-steps', '50', # number of iteration over the whole dataset\n",
        "              '--gpu', '0',\n",
        "              '--gan', 'Vanilla',\n",
        "              '--context_path', 'resnet18', # or 'resnet101'\n",
        "              '--save-pred-every', '2',\n",
        "              '--validation_step', '2',\n",
        "              '--light_discriminator', False,\n",
        "              '--load_pretrained_models', False,\n",
        "              '--pretrained_models_path','/gdrive/MyDrive/Project_AML/Models/checkpoints_segNet/latest_CE_loss.pth',\n",
        "              '--save_models_path', '/gdrive/MyDrive/Project_AML/Models/checkpoints_segNet/',\n",
        "              \n",
        "              '--FDA', 'True',\n",
        "              '--LB', '0.1'\n",
        "\n",
        "\n",
        "\n",
        "    ]\n",
        "    main(params)\n"
      ],
      "metadata": {
        "id": "NBcayZvHNtS-",
        "outputId": "8ceb5310-6a6b-40fc-c830-6cdb48f0f5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GTA:  10\n",
            "Cityscapes:  10\n",
            "GTA image (3, 512, 1024)\n",
            "GTA label (512, 1024)\n",
            "Cityscapes image (3, 512, 1024)\n",
            "Iter_Size =  5\n",
            "iter =        0/      50, loss_seg = 2.084, loss_adv = 0.695, loss_D = 0.693\n",
            " doing validation at iter  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val: 100%|██████████| 20/20 [00:05<00:00,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "precision per pixel for test: 0.016\n",
            "mIoU for validation: 0.005\n",
            " Saving checkpoint in  /gdrive/MyDrive/Project_AML/Models/checkpoints_segNet/ best_CE_loss.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0e1001e99752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     ]\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-0284defa4070>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msourceloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetloaderVal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiou_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_miou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_start_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_start_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m   \u001b[0;31m# final val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetloaderVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-ee40d9ef18f9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, model_D, optimizer, optimizer_D, sourceloader, targetloader, targetloaderVal, iter_size, miou_init, iter_start_i)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mpred_source_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_source_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_source_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_source_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_source_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mloss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_source_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-bee2f428128e>\u001b[0m in \u001b[0;36mloss_calc\u001b[0;34m(pred, labels, gpu, ignore_label)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# out shape batch_size x channels x h x w -> batch_size x channels x h x w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menable_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropy2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_label\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mignore_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}