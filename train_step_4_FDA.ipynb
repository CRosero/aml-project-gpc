{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRosero/aml-project/blob/master/train_step_4_FDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9D12pC0R_eq"
      },
      "source": [
        "# Importing the dataset from drive.\n",
        "( You can find the zipped folder [here](https://drive.google.com/file/d/1XsRmyQYHfgRFJCOueXpJ37yyOCrKHO-W/view?usp=sharing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOzT4IA9ZnnX",
        "outputId": "0d318087-6b6b-4513-f5e3-1cf4aafefc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n",
            "dataset already downloaded\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive/')\n",
        "data_path = \"/content/data\"\n",
        "\n",
        "use_complete_dataset = False\n",
        "\n",
        "\n",
        "if (use_complete_dataset == True) and (not os.path.isfile('/content/data.zip')):\n",
        "  print(\"download entire dataset\")\n",
        "  !gdown --id 1A2dBwPlCyXHTqmG1LRvPfVm6K21jWwUI # 3-5 min\n",
        "  !jar xf  \"/content/data.zip\"\n",
        "elif (use_complete_dataset == False) and (not os.path.isfile('/content/data.zip')):\n",
        "  # Load cropped dataset containing only 10 images\n",
        "  print(\"download cropped dataset\")\n",
        "  !gdown --id 1gPcwDJsNpyqcjKu225hnIxkURpjUA08i   \n",
        "  !jar xf  \"/content/data.zip\"\n",
        "else:\n",
        "  print(\"dataset already downloaded\")\n",
        "\n",
        "if not os.path.isdir('/content/data'):\n",
        "  print(\"Dataset doesn't exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1nqqrU5SIBH"
      },
      "source": [
        "# Cloning the repository from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uzIXW9Z5Kl4",
        "outputId": "6e6d735d-8740-4da7-fa9f-71bd2b69e7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already cloned\n",
            "/content/cloned-repo\n",
            "dataset      image_output  results  train_step_2.ipynb\t    train_step_4.ipynb\n",
            "demo_images  model\t   runs     train_step_3.ipynb\t    utils\n",
            "FDA.ipynb    README.md\t   scripts  train_step_4_FDA.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "repo_path = \"/content/cloned-repo\"\n",
        "if not os.path.isdir(repo_path):\n",
        "  !git clone -l -s https://github.com/CRosero/aml-project.git cloned-repo\n",
        "  %cd cloned-repo\n",
        "else:\n",
        "  print(\"Repository already cloned\")\n",
        "%cd /content/cloned-repo\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4np6uwUgDMW"
      },
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06ImxSTmbCnz",
        "outputId": "4dc72957-07aa-4b52-c7d5-928cb663acf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.6.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QYHTsZf6SkxK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils import data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from utils.utils import reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu, colour_code_segmentation,poly_lr_scheduler\n",
        "from utils.loss import CrossEntropy2d,DiceLoss\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path as osp\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from PIL import Image\n",
        "from torchinfo import summary\n",
        "import json\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Dataset class:\n",
        "from dataset.cityscapesDataSet import cityscapesDataSet\n",
        "from dataset.GTA5DataSet import GTA5DataSet\n",
        "# Discriminator\n",
        "from model.discriminator import FCDiscriminator, LightWeightFCDiscriminator\n",
        "# Network\n",
        "from model.build_BiSeNet import BiSeNet\n",
        "# Validation function\n",
        "from scripts.eval import val\n",
        "# FDA\n",
        "from utils.FDA import FDA_source_to_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "n9TWPNCay2Md",
        "outputId": "0d296a4b-316e-4b50-c0aa-2093c2bcc2de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "evfJ9HAhM9-a"
      },
      "outputs": [],
      "source": [
        "def enable_cuda(obj, gpu):\n",
        "  if torch.cuda.is_available():\n",
        "    return obj.cuda(gpu)\n",
        "  else:\n",
        "    return obj\n",
        "    \n",
        "def loss_calc(pred, labels, gpu, ignore_label=255):\n",
        "    \"\"\"\n",
        "    This function returns cross entropy loss for semantic segmentation\n",
        "    \"\"\"\n",
        "    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n",
        "    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n",
        "    labels = Variable(labels.long()).cuda(gpu)\n",
        "    labels = enable_cuda(labels, gpu)\n",
        "    criterion = CrossEntropy2d(ignore_label= ignore_label)\n",
        "    criterion = enable_cuda(criterion, gpu)\n",
        "\n",
        "    return criterion(pred, labels)\n",
        "\n",
        "\n",
        "def lr_poly(base_lr, iter, max_iter, power):\n",
        "    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n",
        "\n",
        "\n",
        "def adjust_learning_rate(args, optimizer, i_iter):\n",
        "    lr = lr_poly(args.learning_rate, i_iter, args.num_steps, args.power)\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    if len(optimizer.param_groups) > 1:\n",
        "        optimizer.param_groups[1]['lr'] = lr * 10\n",
        "\n",
        "\n",
        "def adjust_learning_rate_D(args, optimizer, i_iter):\n",
        "    lr = lr_poly(args.learning_rate_D, i_iter, args.num_steps, args.power)\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    if len(optimizer.param_groups) > 1:\n",
        "        optimizer.param_groups[1]['lr'] = lr * 10\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, model, model_D, optimizer,optimizer_D, sourceloader, targetloader, targetloaderVal, iter_size, miou_init=0, iter_start_i=0):\n",
        "  # labels for adversarial training\n",
        "  source_label_id = 0\n",
        "  target_label_id = 1\n",
        "\n",
        "  if args.gan == 'Vanilla':\n",
        "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
        "  elif args.gan == 'LS':\n",
        "    bce_loss = torch.nn.MSELoss()\n",
        "\n",
        "  writer = SummaryWriter()\n",
        "  max_miou = miou_init\n",
        "  \n",
        "  if args.FDA:\n",
        "    IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "    IMG_MEAN = torch.reshape( torch.from_numpy(IMG_MEAN), (1,3,1,1)  )\n",
        "    mean_img = torch.zeros(1, 1)\n",
        "\n",
        "  for i_iter in range(iter_start_i, args.num_steps):\n",
        "\n",
        "    loss_seg_value = 0\n",
        "    loss_adv_target_value = 0\n",
        "    loss_D_value = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    adjust_learning_rate(args, optimizer, i_iter)\n",
        "\n",
        "    optimizer_D.zero_grad()\n",
        "    adjust_learning_rate_D(args, optimizer_D, i_iter)\n",
        "    sourceloader_iter = enumerate(sourceloader)\n",
        "    targetloader_iter = enumerate(targetloader)\n",
        "\n",
        "    tq = tqdm(total=iter_size)\n",
        "    tq.set_description('iter %d / %d' % (i_iter, args.num_steps))\n",
        "    \n",
        "    for sub_i in range(iter_size): \n",
        "        # train G\n",
        "        model.train()        \n",
        "        # don't accumulate grads in D\n",
        "        for param in model_D.parameters():\n",
        "            param.requires_grad = False\n",
        "        # get batch from dataloaders\n",
        "        _, batch_source = next(sourceloader_iter)  # new batch source\n",
        "        source_images, source_labels = batch_source\n",
        "        _, batch_target = next(targetloader_iter) # new batch target\n",
        "        target_images, _ = batch_target\n",
        "\n",
        "        if args.FDA: \n",
        "          if mean_img.shape[-1] < 2:\n",
        "              B, C, H, W = source_images.shape\n",
        "              mean_img = IMG_MEAN.repeat(B,1,H,W)\n",
        "          #-------------------------------------------------------------------#\n",
        "          # 1. source to target, target to target\n",
        "          src_in_trg = FDA_source_to_target( source_images, target_images, L=args.LB )    # src_lbl\n",
        "          trg_in_trg = target_images\n",
        "          # 2. subtract mean\n",
        "          source_images = src_in_trg.clone() - mean_img   # src, src_lbl\n",
        "          target_images = trg_in_trg.clone() - mean_img   # trg, trg_lbl\n",
        "\n",
        "          #-------------------------------------------------------------------#\n",
        "        \n",
        "        # train with source images and labels\n",
        "        source_images = Variable(source_images)\n",
        "        source_images = enable_cuda(source_images, args.gpu)\n",
        "        source_labels = Variable(source_labels)\n",
        "        source_labels = enable_cuda(source_labels, args.gpu)\n",
        "\n",
        "        pred_source_result, pred_source_1, pred_source_2 = model(source_images)\n",
        "        loss1 = loss_calc(pred_source_result, source_labels, args.gpu, args.ignore_label)\n",
        "        loss2 = loss_calc(pred_source_1, source_labels, args.gpu, args.ignore_label)\n",
        "        loss3 = loss_calc(pred_source_2, source_labels, args.gpu, args.ignore_label)\n",
        "        loss_seg = loss1 + loss2 + loss3\n",
        "\n",
        "        # proper normalization\n",
        "        loss_seg = loss_seg / iter_size\n",
        "        loss_seg.backward()\n",
        "        #print(\"Segmentation loss:\", loss_seg.data.cpu().numpy())\n",
        "        #print(loss_seg.data.cpu().numpy().shape)\n",
        "        loss_seg_value += loss_seg.data.cpu().numpy() / iter_size\n",
        "\n",
        "        # train with target images\n",
        "        target_images = Variable(target_images).cuda(args.gpu)\n",
        "\n",
        "        pred_target_result, pred_target_1, pred_target_2 = model(target_images)\n",
        "\n",
        "        # generator vs. discriminator \n",
        "        D_out = model_D(F.softmax(pred_target_result))\n",
        "\n",
        "        loss_adv_target = bce_loss(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label_id)).cuda(args.gpu))\n",
        "        loss = args.lambda_adv_target * loss_adv_target\n",
        "        loss = loss / iter_size\n",
        "        loss.backward()\n",
        "        loss_adv_target_value += loss_adv_target.data.cpu().numpy() / iter_size\n",
        "\n",
        "        # train discriminator\n",
        "        # bring back requires_grad\n",
        "        for param in model_D.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # train with source\n",
        "        pred_source_result = pred_source_result.detach()\n",
        "        \n",
        "        D_out_source = model_D(F.softmax(pred_source_result))\n",
        "        loss_D_source = bce_loss(D_out_source, Variable(torch.FloatTensor(D_out_source.data.size()).fill_(source_label_id)).cuda(args.gpu))\n",
        "        loss_D_source = loss_D_source / iter_size / 2\n",
        "        loss_D_source.backward()\n",
        "\n",
        "        loss_D_value += loss_D_source.data.cpu().numpy()\n",
        "\n",
        "        # train with target\n",
        "        pred_target_result = pred_target_result.detach()\n",
        "\n",
        "        D_out_target = model_D(F.softmax(pred_target_result))\n",
        "\n",
        "        loss_D_target = bce_loss(D_out_target, Variable(torch.FloatTensor(D_out_target.data.size()).fill_(target_label_id)).cuda(args.gpu))\n",
        "        loss_D_target = loss_D_target / iter_size / 2\n",
        "        loss_D_target.backward()\n",
        "\n",
        "        loss_D_value += loss_D_target.data.cpu().numpy()\n",
        "        tq.update(args.batch_size)\n",
        "    \n",
        "    tq.close()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    writer.add_scalar('loss_seg_value', loss_seg_value, i_iter)\n",
        "    writer.add_scalar('loss_adv_target_value', loss_adv_target_value, i_iter)\n",
        "    writer.add_scalar('loss_D_value', loss_D_value, i_iter)\n",
        "    print('iter = {0:8d}/{1:8d}, loss_seg = {2:.3f}, loss_adv = {3:.3f}, loss_D = {4:.3f}'.format(i_iter, args.num_steps, loss_seg_value, loss_adv_target_value, loss_D_value))\n",
        "\n",
        "\n",
        "    if i_iter % args.save_pred_every == 0 and i_iter != 0:\n",
        "        print(\" Saving checkpoint in \", args.save_models_path, \"latest_CE_loss.pth\")\n",
        "        if not os.path.isdir(args.save_models_path):\n",
        "            os.mkdir(args.save_models_path)\n",
        "        torch.save({\n",
        "                  'iter': i_iter,\n",
        "                  'segNet_state_dict': model.state_dict(),\n",
        "                  'D_state_dict': model_D.state_dict(),\n",
        "                  'optimizer_seg_state_dict': optimizer.state_dict(),\n",
        "                  'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "                  'max_miou' : max_miou,\n",
        "                    },\n",
        "                    os.path.join(args.save_models_path, 'latest_CE_loss.pth'))\n",
        "\n",
        "    \n",
        "    if i_iter % args.validation_step == 0:\n",
        "        print(\" doing validation at iter \", i_iter)\n",
        "        precision, miou = val(args, model, targetloaderVal)\n",
        "        if miou > max_miou:\n",
        "            max_miou = miou\n",
        "            print(\" Saving checkpoint in \", args.save_models_path, \"best_CE_loss.pth\")\n",
        "            os.makedirs(args.save_models_path, exist_ok=True)\n",
        "            torch.save({\n",
        "                  'iter': i_iter,\n",
        "                  'segNet_state_dict': model.state_dict(),\n",
        "                  'D_state_dict': model_D.state_dict(),\n",
        "                  'optimizer_seg_state_dict': optimizer.state_dict(),\n",
        "                  'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "                  'max_miou' : max_miou,\n",
        "                    },\n",
        "                    os.path.join(args.save_models_path, 'best_CE_loss.pth'))\n",
        "        writer.add_scalar('precision', precision, i_iter)\n",
        "        writer.add_scalar('miou', miou, i_iter)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "xdrFawWmP8qE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "S24rDhec9N0b"
      },
      "outputs": [],
      "source": [
        "def get_arguments(params=[]):\n",
        "    \"\"\"Parse all the arguments provided from the CLI.\n",
        "    Returns:\n",
        "      A list of parsed arguments.\n",
        "    \"\"\"\n",
        "        \n",
        "    # basic parameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default='BiseNet',\n",
        "                        help=\"available options : BiseNet\")\n",
        "    parser.add_argument(\"--target\", type=str, default='Cityscapes',\n",
        "                        help=\"available options : Cityscapes\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=2,\n",
        "                        help=\"Number of images sent to the network in one step.\")\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=4,\n",
        "                        help=\"number of workers for multithread dataloading.\")\n",
        "    parser.add_argument(\"--data-dir\", type=str, default='',\n",
        "                        help=\"Path to the directory containing the source dataset.\")\n",
        "    parser.add_argument(\"--ignore-label\", type=int, default= 255,\n",
        "                        help=\"The index of the label to ignore during the training.\")\n",
        "    parser.add_argument(\"--input-size\", type=str, default='1024,512',\n",
        "                        help=\"Comma-separated string with height and width of source images.\")\n",
        "    parser.add_argument(\"--input-size-target\", type=str, default='1024,512',\n",
        "                        help=\"Comma-separated string with height and width of target images.\")\n",
        "    parser.add_argument(\"--is-training\", action=\"store_true\",\n",
        "                        help=\"Whether to updates the running means and variances during the training.\")\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-2,\n",
        "                        help=\"Base learning rate for training with polynomial decay.\")\n",
        "    parser.add_argument(\"--learning-rate-D\", type=float, default=1e-4,\n",
        "                        help=\"Base learning rate for discriminator.\")\n",
        "    parser.add_argument(\"--lambda-seg\", type=float, default=1,\n",
        "                        help=\"lambda_seg.\")\n",
        "    parser.add_argument(\"--lambda-adv-target\", type=float, default=0.001,\n",
        "                        help=\"lambda_adv for adversarial training.\")\n",
        "    parser.add_argument(\"--momentum\", type=float, default=0.9,\n",
        "                        help=\"Momentum component of the optimiser.\")\n",
        "    parser.add_argument(\"--num-classes\", type=int, default=19,\n",
        "                        help=\"Number of classes to predict (including background).\")\n",
        "    parser.add_argument(\"--num-steps\", type=int, default=50,\n",
        "                        help=\"Number of training steps.\")\n",
        "    parser.add_argument(\"--num-steps-stop\", type=int, default=150,\n",
        "                        help=\"Number of training steps for early stopping.\")\n",
        "    parser.add_argument(\"--power\", type=float, default=0.9,\n",
        "                        help=\"Decay parameter to compute the learning rate.\")\n",
        "    parser.add_argument(\"--random-mirror\", action=\"store_true\",\n",
        "                        help=\"Whether to randomly mirror the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-scale\", action=\"store_true\",\n",
        "                        help=\"Whether to randomly scale the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-seed\", type=int, default=42,\n",
        "                        help=\"Random seed to have reproducible results.\")\n",
        "    parser.add_argument(\"--save-pred-every\", type=int, default=10,\n",
        "                        help=\"Save summaries and checkpoint every often.\")\n",
        "    parser.add_argument(\"--weight-decay\", type=float, default=1e-4,\n",
        "                        help=\"Regularisation parameter for L2-loss.\")\n",
        "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
        "                        help=\"choose gpu device.\")\n",
        "    parser.add_argument(\"--gan\", type=str, default='Vanilla',\n",
        "                        help=\"choose the GAN objective.\")\n",
        "    parser.add_argument('--context_path', type=str, default='resnet18',\n",
        "                        help='The context path model you are using, resnet18, resnet101.')\n",
        "    parser.add_argument('--validation_step', type=int, default=10, help='How often to perform validation (epochs)')\n",
        "    parser.add_argument('--use_gpu', type=bool, default=True, help='whether to user gpu for training')\n",
        "    parser.add_argument(\"--light_discriminator\", type=bool, default=False, \n",
        "                        help=\"using discriminator with lightweight depthwise-separable convolutions\")\n",
        "    parser.add_argument('--load_pretrained_models', type=bool, default=False, help='load or not the pretrained models from the saved checkpoint ')\n",
        "    parser.add_argument('--pretrained_models_path', type=str, default=\"\", help='path to pretrained models')\n",
        "    parser.add_argument('--save_models_path', type=str, default=\"\", help='path to save models')\n",
        "    \n",
        "    parser.add_argument('--FDA', type=bool, default=True, help='whether to use FDA to transform source images')\n",
        "    parser.add_argument(\"--LB\", type=float, default=0.1, help=\"beta for FDA\")\n",
        "\n",
        "\n",
        "\n",
        "    args = parser.parse_args(params)\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(params):\n",
        "  \"\"\"Create the model and start the training.\"\"\"\n",
        "  args = get_arguments(params)\n",
        "\n",
        "  # Set random seed\n",
        "  torch.manual_seed(args.random_seed)\n",
        "  torch.cuda.manual_seed(args.random_seed)\n",
        "  np.random.seed(args.random_seed)\n",
        "  random.seed(args.random_seed)\n",
        "\n",
        "  # input sizes\n",
        "  w, h = map(int, args.input_size.split(','))\n",
        "  input_size = (w, h)\n",
        "\n",
        "  w, h = map(int, args.input_size_target.split(','))\n",
        "  input_size_target = (w, h)\n",
        "\n",
        "  cudnn.benchmark = True\n",
        "  cudnn.enabled = True\n",
        "  gpu = args.gpu\n",
        "\n",
        "  # Create network\n",
        "  if args.model == 'BiseNet':\n",
        "    model = BiSeNet(num_classes=args.num_classes, context_path= args.context_path)\n",
        "\n",
        "  # Set the network to train mode\n",
        "  model.train()\n",
        "  model = enable_cuda(model, args.gpu)\n",
        "\n",
        "  # init D\n",
        "  if args.light_discriminator == False:\n",
        "    print(\"Using a fully convolutional discriminator\")\n",
        "    model_D = FCDiscriminator(num_classes=args.num_classes)\n",
        "  else:\n",
        "    print(\"Using a discriminator with lightweight depthwise-separable convolution\")\n",
        "    model_D = LightWeightFCDiscriminator(num_classes=args.num_classes)\n",
        "\n",
        "  # Set the discriminators to train mode\n",
        "  model_D.train()\n",
        "  model_D = enable_cuda(model_D, args.gpu)\n",
        "\n",
        "  \n",
        "  ''' \n",
        "  # Printing statistics\n",
        "  print(\"Segmentation Network\\n\")\n",
        "  print(summary(enable_cuda(model.eval(), args.gpu), input_size=(BATCH_SIZE, 3, input_size[0], input_size[1])))\n",
        "\n",
        "  if args.light_discriminator == False: \n",
        "    print(\"Adversarial discriminator Architecture\\n\")\n",
        "  else:\n",
        "    print(\"Lightweight Adversarial Domain Adaptation\\n\")\n",
        "  print(summary(enable_cuda(model_D.eval(), args.gpu), input_size=(BATCH_SIZE, 19, input_size[0], input_size[1])))\n",
        "  '''\n",
        "  # Path\n",
        "  source_data_root_path = os.path.join(args.data_dir, \"GTA5\") # /content/data/GTA5\n",
        "  target_data_root_path = os.path.join(args.data_dir, args.target) # /content/data/Cityscapes\n",
        "  source_train_path = os.path.join(source_data_root_path, \"train.txt\") # /content/data/GTA5/train.txt\n",
        "  target_root_path = os.path.join(target_data_root_path,  \"train.txt\")   # /content/data/Cityscapes/train.txt\n",
        "  info_path = os.path.join(source_data_root_path,  \"info.json\") # /content/data/GTA/info.json \n",
        "\n",
        "  info_json = json.load(open(info_path))\n",
        "\n",
        "  # Image mean\n",
        "  IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "  # Zero mean\n",
        "  IMG_MEAN_ZERO = np.array((0.0, 0.0, 0.0), dtype=np.float32)\n",
        "\n",
        "  if (args.FDA):\n",
        "    img_mean = IMG_MEAN_ZERO # From FDA original code: \"use the original images for FDA, then do mean subtraction, normalization, etc. Otherwise, will be numerical artifact\"\n",
        "    IMG_MEAN = torch.reshape( torch.from_numpy(IMG_MEAN), (1,3,1,1)  )\n",
        "  else:\n",
        "    img_mean = IMG_MEAN\n",
        "\n",
        "  # Datasets  \n",
        "  source_dataset = GTA5DataSet(source_data_root_path, source_train_path, info_json, crop_size=input_size, mean=img_mean)\n",
        "  target_dataset = cityscapesDataSet(target_data_root_path, target_root_path, info_json, crop_size=input_size_target, mean=img_mean)\n",
        "\n",
        "  print(\"GTA: \", len(source_dataset))\n",
        "  print(\"Cityscapes: \", len(source_dataset))\n",
        "  img,label = source_dataset[0]\n",
        "  print (\"GTA image\", img.shape )\n",
        "  print (\"GTA label\", label.shape )\n",
        "  img, _ = target_dataset[0]\n",
        "  print (\"Cityscapes image\", img.shape )\n",
        "\n",
        "  # Itersize\n",
        "  assert len(source_dataset)==len(target_dataset)\n",
        "  iter_size = len(source_dataset) // args.batch_size # the source and the target have the same len\n",
        "  print(\"Iter_Size = \", iter_size)\n",
        "\n",
        "  # Create DataLoaders\n",
        "  sourceloader = data.DataLoader(source_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "  targetloader = data.DataLoader(target_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "  targetloaderVal = data.DataLoader(target_dataset, batch_size=1, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "  # Optimizer\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  optimizer_D = optim.Adam(model_D.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n",
        "  optimizer_D.zero_grad()\n",
        "\n",
        "  # to keep track of best miou\n",
        "  max_miou = 0\n",
        "  # start iteration from:\n",
        "  iter_start_i = 0\n",
        "\n",
        "  # load pretrained model if exists\n",
        "  checkpoint = None\n",
        "  if (args.load_pretrained_models) and (args.pretrained_models_path is not None) and (os.path.isfile(args.pretrained_models_path)):\n",
        "      print('load models from %s ...' % args.pretrained_models_path)\n",
        "      checkpoint= torch.load(args.pretrained_models_path)\n",
        "      model.load_state_dict(checkpoint['segNet_state_dict'])\n",
        "      model_D.load_state_dict(checkpoint['D_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_seg_state_dict'])\n",
        "      optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "      iter_start_i = int(checkpoint['iter'])+1\n",
        "      max_miou = float(checkpoint['max_miou']) \n",
        "      print('Done! Loaded model trained until iter:', iter_start_i, \"best miou so far:\", max_miou)\n",
        "\n",
        "  # train\n",
        "  train(args, model, model_D, optimizer,optimizer_D, sourceloader, targetloader, targetloaderVal,iter_size, miou_init=max_miou, iter_start_i=iter_start_i)\n",
        "  # final val\n",
        "  val(args, model, targetloaderVal, save=True)"
      ],
      "metadata": {
        "id": "AH9nIjoyN4fM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    params = [\n",
        "              '--model', 'BiseNet',\n",
        "              '--target', 'Cityscapes',\n",
        "              '--batch-size', '2',\n",
        "              '--num-workers', '4',\n",
        "              '--data-dir', '/content/data/',\n",
        "              '--ignore-label', '255',\n",
        "              '--input-size', '1024,512',\n",
        "              '--input-size-target',  '1024,512',\n",
        "              '--learning-rate', '2.5e-2',\n",
        "              '--learning-rate-D', '1e-4',\n",
        "              '--lambda-seg', '1',\n",
        "              '--lambda-adv-target', '0.001',\n",
        "              '--momentum', '0.9',\n",
        "              '--power', '0.9',\n",
        "              '--weight-decay','1e-4',\n",
        "              '--num-classes', '19',\n",
        "              '--num-steps', '50', # number of iteration over the whole dataset\n",
        "              '--gpu', '0',\n",
        "              '--gan', 'Vanilla',\n",
        "              '--context_path', 'resnet18', # or 'resnet101'\n",
        "              '--save-pred-every', '2',\n",
        "              '--validation_step', '2',\n",
        "              '--light_discriminator', False,\n",
        "              '--load_pretrained_models', False,\n",
        "              '--pretrained_models_path','/gdrive/MyDrive/Project_AML/Models/checkpoints_segNet/latest_CE_loss.pth',\n",
        "              '--save_models_path', '/gdrive/MyDrive/Project_AML/Models/checkpoints_segNet/',\n",
        "              \n",
        "              '--FDA', 'True',\n",
        "              '--LB', '0.1'\n",
        "\n",
        "\n",
        "\n",
        "    ]\n",
        "    main(params)\n"
      ],
      "metadata": {
        "id": "NBcayZvHNtS-",
        "outputId": "d1b322b4-b517-4f20-8235-8e4b07721165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a fully convolutional discriminator\n",
            "GTA:  10\n",
            "Cityscapes:  10\n",
            "GTA image (3, 512, 1024)\n",
            "GTA label (512, 1024)\n",
            "Cityscapes image (3, 512, 1024)\n",
            "Iter_Size =  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "iter 0 / 50:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "iter 0 / 50:  40%|████      | 2/5 [00:05<00:07,  2.51s/it]\u001b[A\n",
            "iter 0 / 50:  80%|████████  | 4/5 [00:07<00:01,  1.75s/it]\u001b[A\n",
            "iter 0 / 50: : 6it [00:10,  1.65s/it]                     \u001b[A\n",
            "iter 0 / 50: : 8it [00:13,  1.61s/it]\u001b[A\n",
            "iter 0 / 50: : 10it [00:16,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter =        0/      50, loss_seg = 2.110, loss_adv = 0.695, loss_D = 0.693\n",
            " doing validation at iter  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "val:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "val:  10%|█         | 2/20 [00:00<00:07,  2.56it/s]\u001b[A\n",
            "val:  20%|██        | 4/20 [00:01<00:06,  2.33it/s]\u001b[A\n",
            "val:  30%|███       | 6/20 [00:02<00:04,  3.19it/s]\u001b[A\n",
            "val:  40%|████      | 8/20 [00:02<00:02,  4.18it/s]\u001b[A\n",
            "val:  50%|█████     | 10/20 [00:02<00:01,  5.06it/s]\u001b[A\n",
            "val:  60%|██████    | 12/20 [00:02<00:01,  5.83it/s]\u001b[A\n",
            "val:  70%|███████   | 14/20 [00:03<00:00,  6.32it/s]\u001b[A\n",
            "val:  80%|████████  | 16/20 [00:03<00:00,  6.60it/s]\u001b[A\n",
            "val:  90%|█████████ | 18/20 [00:03<00:00,  6.95it/s]\u001b[A\n",
            "val: 100%|██████████| 20/20 [00:04<00:00,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "precision per pixel for test: 0.011\n",
            "mIoU for validation: 0.007\n",
            " Saving checkpoint in  /gdrive/MyDrive/Project_AML/Models/checkpoints_segNet/ best_CE_loss.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "iter 1 / 50:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "iter 1 / 50:  40%|████      | 2/5 [00:04<00:06,  2.02s/it]\u001b[A\n",
            "iter 1 / 50:  80%|████████  | 4/5 [00:06<00:01,  1.56s/it]\u001b[A\n",
            "iter 1 / 50: : 6it [00:08,  1.35s/it]                     \u001b[A"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b4a80191ceb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     ]\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-af04559c1884>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msourceloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetloaderVal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiou_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_miou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_start_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_start_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m   \u001b[0;31m# final val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetloaderVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-60f2ba6bb07b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, model_D, optimizer, optimizer_D, sourceloader, targetloader, targetloaderVal, iter_size, miou_init, iter_start_i)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# proper normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mloss_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_seg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0miter_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mloss_seg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;31m#print(\"Segmentation loss:\", loss_seg.data.cpu().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#print(loss_seg.data.cpu().numpy().shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}