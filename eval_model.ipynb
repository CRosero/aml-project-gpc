{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRosero/aml-project-gpc/blob/master/eval_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9D12pC0R_eq"
      },
      "source": [
        "# Importing the dataset from drive.\n",
        "( You can find the zipped folder [here](https://drive.google.com/file/d/1XsRmyQYHfgRFJCOueXpJ37yyOCrKHO-W/view?usp=sharing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOzT4IA9ZnnX",
        "outputId": "11793cbf-03ef-40b3-9dfb-f156f26268f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n",
            "dataset already downloaded\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive/')\n",
        "data_path = \"/content/data\"\n",
        "\n",
        "use_complete_dataset = False\n",
        "\n",
        "\n",
        "if (use_complete_dataset == True) and (not os.path.isfile('/content/data.zip')):\n",
        "  print(\"download entire dataset\")\n",
        "  !gdown --id 1A2dBwPlCyXHTqmG1LRvPfVm6K21jWwUI # 3-5 min\n",
        "  !jar xf  \"/content/data.zip\"\n",
        "elif (use_complete_dataset == False) and (not os.path.isfile('/content/data.zip')):\n",
        "  # Load cropped dataset containing only 10 images\n",
        "  print(\"download cropped dataset\")\n",
        "  !gdown --id 1gPcwDJsNpyqcjKu225hnIxkURpjUA08i   \n",
        "  !jar xf  \"/content/data.zip\"\n",
        "else:\n",
        "  print(\"dataset already downloaded\")\n",
        "\n",
        "if not os.path.isdir('/content/data'):\n",
        "  print(\"Dataset doesn't exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1nqqrU5SIBH"
      },
      "source": [
        "# Cloning the repository from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uzIXW9Z5Kl4",
        "outputId": "d64b1b3b-2917-4af4-8e37-5bdbf75bd1f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repository already cloned\n",
            "/content/cloned-repo\n",
            " dataset        runs\n",
            " demo_images    train.py\n",
            " eval.ipynb     train_step_2-experiment.ipynb\n",
            " eval.py        train_step_2.ipynb\n",
            " image_output   train_step_3.ipynb\n",
            " loss.py        train_step_4.FDA_TENSOR.ipynb\n",
            " model\t        train_step_4.ipynb\n",
            " __pycache__   'Train using corrected input size'\n",
            " README.md      utils.py\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "repo_path = \"/content/cloned-repo\"\n",
        "if not os.path.isdir(repo_path):\n",
        "  !git clone -l -s https://github.com/CRosero/aml-project.git cloned-repo\n",
        "  %cd cloned-repo\n",
        "else:\n",
        "  print(\"Repository already cloned\")\n",
        "%cd /content/cloned-repo\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4np6uwUgDMW"
      },
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYHTsZf6SkxK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils import data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from utils.utils import reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu, colour_code_segmentation,poly_lr_scheduler\n",
        "from utils.loss import CrossEntropy2d,DiceLoss\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path as osp\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from PIL import Image\n",
        "#from torchinfo import summary\n",
        "#from fvcore.nn import FlopCountAnalysis\n",
        "import json\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Dataset class:\n",
        "from dataset.cityscapesDataSet import cityscapesDataSet\n",
        "from dataset.GTA5DataSet import GTA5DataSet\n",
        "# Discriminator\n",
        "from model.discriminator import FCDiscriminator, LightWeightFCDiscriminator\n",
        "# Network\n",
        "from model.build_BiSeNet import BiSeNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val(args, model, dataloader, save=False, batch_size=1):\n",
        "\n",
        "    #TODO: prendere dal json\n",
        "    palette = [[128,64,128],[244,35,232], [70,70,70],[102,102,156],[190,153,153],[153,153,153],[250,170,30],[220,220,0],[107,142,35],[152,251,152],[70,130,180],[220,20,60],[255,0,0],[0,0,142],[0,0,70],[0,60,100],[0,80,100],[0,0,230],[119,11,32],[0,0,0]]\n",
        "    num = list(range(0, len(palette)-1))\n",
        "    num.append(255)\n",
        "    dictionary = dict(zip(num, palette)) \n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        precision_record = []\n",
        "        hist = np.zeros((args.num_classes, args.num_classes))\n",
        "        tq =tqdm(total=len(dataloader) * batch_size)\n",
        "        tq.set_description('val')\n",
        "        \n",
        "        for i, (data, label) in enumerate(dataloader):\n",
        "            tq.update(batch_size)\n",
        "            label = label.type(torch.LongTensor)\n",
        "            if torch.cuda.is_available() and args.use_gpu:\n",
        "                data = data.cuda()\n",
        "                label = label.cuda()\n",
        "\n",
        "            # get RGB predict image\n",
        "            predict = model(data).squeeze()\n",
        "            predict = reverse_one_hot(predict)\n",
        "            predict = np.array(predict.cpu())\n",
        "\n",
        "            # get RGB label image\n",
        "            label = label.squeeze()\n",
        "            label = np.array(label.cpu())\n",
        "\n",
        "            # compute per pixel accuracy\n",
        "            precision = compute_global_accuracy(predict, label)\n",
        "            hist += fast_hist(label.flatten(), predict.flatten(), args.num_classes)\n",
        "\n",
        "            precision_record.append(precision)\n",
        "\n",
        "            if save and i < 20:\n",
        "              # save some images\n",
        "              predict = colour_code_segmentation(np.array(predict), dictionary)\n",
        "              label = colour_code_segmentation(np.array(label), dictionary)\n",
        "              if not os.path.isdir(\"/content/cloned-repo/image_output\"):\n",
        "                os.mkdir(\"/content/cloned-repo/image_output\")\n",
        "\n",
        "              if not os.path.isdir(\"/content/cloned-repo/image_output/predict\"):\n",
        "                os.mkdir(\"/content/cloned-repo/image_output/predict\")\n",
        "\n",
        "              if not os.path.isdir(\"/content/cloned-repo/image_output/label\"):\n",
        "                os.mkdir(\"/content/cloned-repo/image_output/label\")\n",
        "\n",
        "              predictImage = Image.fromarray(predict.astype('uint8'), \"RGB\")\n",
        "              predictImage.save(\"/content/cloned-repo/image_output/predict/\" + str(i) + \".png\")\n",
        "\n",
        "              labelImage = Image.fromarray(label.astype('uint8'), \"RGB\")\n",
        "              labelImage.save(\"/content/cloned-repo/image_output/label/\" + str(i) + \".png\")\n",
        "            \n",
        "        \n",
        "        precision = np.mean(precision_record)\n",
        "        # miou = np.mean(per_class_iu(hist))\n",
        "        miou_list = per_class_iu(hist)[:-1]\n",
        "        # miou_dict, miou = cal_miou(miou_list, csv_path)\n",
        "        miou = np.mean(miou_list)\n",
        "        print(\"\")\n",
        "        print('precision per pixel for test: %.3f' % precision)\n",
        "        print('mIoU for validation: %.3f' % miou)\n",
        "        # miou_str = ''\n",
        "        # for key in miou_dict:\n",
        "        #     miou_str += '{}:{},\\n'.format(key, miou_dict[key])\n",
        "        # print('mIoU for each class:')\n",
        "        # print(miou_str)\n",
        "        return precision, miou\n",
        "\n",
        "  def test(model,dataloader, info_json, save=False, batch_size=1):\n",
        "    #TODO: prendere dal json\n",
        "    #palette = [[128,64,128],[244,35,232], [70,70,70],[102,102,156],[190,153,153],[153,153,153],[250,170,30],[220,220,0],[107,142,35],[152,251,152],[70,130,180],[220,20,60],[255,0,0],[0,0,142],[0,0,70],[0,60,100],[0,80,100],[0,0,230],[119,11,32],[0,0,0]]\n",
        "    palette = info_json['palette']\n",
        "    num = list(range(0, len(palette)-1))\n",
        "    num.append(255)\n",
        "    dictionary = dict(zip(num, palette)) \n",
        "    print('start test!')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        precision_record = []\n",
        "        tq = tqdm.tqdm(total=len(dataloader) * args.batch_size)\n",
        "        tq.set_description('test')\n",
        "        hist = np.zeros((args.num_classes, args.num_classes))\n",
        "\n",
        "        for i, (data, label) in enumerate(dataloader):\n",
        "            tq.update(args.batch_size)\n",
        "            if torch.cuda.is_available() and args.use_gpu:\n",
        "                data = data.cuda()\n",
        "                label = label.cuda()\n",
        "\n",
        "            # get RGB predict image\n",
        "            predict = model(data).squeeze()\n",
        "            predict = reverse_one_hot(predict)\n",
        "            predict = np.array(predict)\n",
        "            # predict = colour_code_segmentation(np.array(predict), label_info)\n",
        "\n",
        "            # get RGB label image\n",
        "            label = label.squeeze()\n",
        "            label = np.array(label)\n",
        "            # label = colour_code_segmentation(np.array(label), label_info)\n",
        "\n",
        "            # compute per pixel accuracy\n",
        "            precision = compute_global_accuracy(predict, label)\n",
        "            hist += fast_hist(label.flatten(), predict.flatten(), args.num_classes)\n",
        "            \n",
        "            precision_record.append(precision)\n",
        "\n",
        "            if save and i < 20:\n",
        "              # save some images\n",
        "              predict = colour_code_segmentation(np.array(predict), dictionary)\n",
        "              label = colour_code_segmentation(np.array(label), dictionary)\n",
        "              if not os.path.isdir(\"/content/cloned-repo/image_output\"):\n",
        "                os.mkdir(\"/content/cloned-repo/image_output\")\n",
        "\n",
        "              if not os.path.isdir(\"/content/cloned-repo/image_output/predict\"):\n",
        "                os.mkdir(\"/content/cloned-repo/image_output/predict\")\n",
        "\n",
        "              if not os.path.isdir(\"/content/cloned-repo/image_output/label\"):\n",
        "                os.mkdir(\"/content/cloned-repo/image_output/label\")\n",
        "\n",
        "              predictImage = Image.fromarray(predict.astype('uint8'), \"RGB\")\n",
        "              predictImage.save(\"/content/cloned-repo/image_output/predict/\" + str(i) + \".png\")\n",
        "\n",
        "              labelImage = Image.fromarray(label.astype('uint8'), \"RGB\")\n",
        "              labelImage.save(\"/content/cloned-repo/image_output/label/\" + str(i) + \".png\")\n",
        "        \n",
        "        precision = np.mean(precision_record)\n",
        "        miou_list = per_class_iu(hist)[:-1]\n",
        "        miou_dict, miou = cal_miou(miou_list, info_json)\n",
        "        print('IoU for each class:')\n",
        "        for key in miou_dict:\n",
        "            print('{}:{},'.format(key, miou_dict[key]))\n",
        "        tq.close()\n",
        "        print('precision for test: %.3f' % precision)\n",
        "        print('mIoU for validation: %.3f' % miou)\n",
        "        return precision"
      ],
      "metadata": {
        "id": "jFutbOGF-blj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arguments(params=[]):\n",
        "    \"\"\"Parse all the arguments provided from the CLI.\n",
        "    Returns:\n",
        "      A list of parsed arguments.\n",
        "    \"\"\"   \n",
        "    \n",
        "    # basic parameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument('--dataset', type=str, default=\"Cityscapes\", help='Dataset you are using.')\n",
        "    parser.add_argument('--crop_width', type=int, default=1024, help='Width of cropped/resized input image to network')\n",
        "    parser.add_argument('--crop_height', type=int, default=512, help='Height of cropped/resized input image to network')   \n",
        "\n",
        "    parser.add_argument('--context_path', type=str, default=\"resnet101\",\n",
        "                        help='The context path model you are using, resnet18, resnet101.')    \n",
        "    parser.add_argument('--data', type=str, default='content/data', help='path of training data')\n",
        "    parser.add_argument('--num_workers', type=int, default=8, help='num of workers')\n",
        "    parser.add_argument('--num_classes', type=int, default=32, help='num of object classes (with void)')\n",
        "    parser.add_argument('--cuda', type=str, default='0', help='GPU ids used for training')\n",
        "    parser.add_argument('--use_gpu', type=bool, default=True, help='whether to user gpu for training')\n",
        "    parser.add_argument('--pretrained_model_path', type=str, default=None, help='path to pretrained model')   \n",
        "    parser.add_argument(\"--random-seed\", type=int, default=42, help=\"Random seed to have reproducible results.\")\n",
        "\n",
        "    \n",
        "    args = parser.parse_args(params)\n",
        "    return args"
      ],
      "metadata": {
        "id": "RfoWoDFvECTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(params):\n",
        "\n",
        "    args = get_arguments(params)\n",
        "    \n",
        "    # Set random seed\n",
        "    torch.manual_seed(args.random_seed)\n",
        "    torch.cuda.manual_seed(args.random_seed)\n",
        "    np.random.seed(args.random_seed)\n",
        "    random.seed(args.random_seed)\n",
        "\n",
        "    # create dataset and dataloader\n",
        "    data_root_path = os.path.join(args.data, args.dataset) # /content/data/Cityscapes\n",
        "    val_path = os.path.join(data_root_path, \"val.txt\")   # /content/data/Cityscapes/val.txt\n",
        "    info_path = os.path.join(args.data, args.dataset, \"info.json\") # /content/data/Cityscapes/info.json \n",
        "    \n",
        "    # preprocessing informations:\n",
        "    input_size = (int(args.crop_width), int(args.crop_height))\n",
        "    f = open(info_path)\n",
        "    info = json.load(f)\n",
        "    img_mean = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "    img_mean = np.array(img_mean, dtype=np.float32)\n",
        "    \n",
        "    # augmentation values\n",
        "    #AUG_VALUES = None\n",
        "    AUG_VALUES = {\n",
        "        \"prob\" : 0.5, \n",
        "        \"kernel_size\" : 9,\n",
        "        \"sigma\" : (1,2)\n",
        "    }\n",
        "\n",
        "    \n",
        "    test_dataset = cityscapesDataSet(root=data_root_path,\n",
        "                                    list_path = val_path,\n",
        "                                    info_json = info,\n",
        "                                    crop_size=input_size, \n",
        "                                    mean=img_mean)\n",
        "\n",
        "    print(f'test_dataset: {len(test_dataset)}')\n",
        "    image, label = test_dataset[0]\n",
        "    print(f'images shape: {image.shape}')\n",
        "    print(f'label shape: {label.shape}')\n",
        "    \n",
        "    # Define dataloaders\n",
        "\n",
        "    dataloader_test = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "    \n",
        "    # build model\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.cuda\n",
        "    \n",
        "    model = BiSeNet(args.num_classes, args.context_path)\n",
        "    \n",
        "    if torch.cuda.is_available() and args.use_gpu:\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "\n",
        "    # load pretrained model if exists\n",
        "    if (args.load_pretrained_model) and (args.pretrained_model_path is not None) and (os.path.isfile(args.pretrained_model_path)):\n",
        "        print('load model from %s ...' % args.pretrained_model_path)\n",
        "        checkpoint= torch.load(args.pretrained_model_path)\n",
        "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch_start_i = int(checkpoint['epoch'])+1\n",
        "        miou_init = float(checkpoint['max_miou'])\n",
        "        print('Done!')\n",
        "        print('Trained until Epoch:', epoch_start_i)\n",
        "        print('- Best miou:', miou_init)\n",
        "\n",
        "    # train\n",
        "    test(args, model, dataloader_test,info, save=True, batch_size=1)"
      ],
      "metadata": {
        "id": "bOofvAwgDZ5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    params = [\n",
        "        '--pretrained_model_path', 'path/to/ckpt',\n",
        "        '--data', 'content/data',\n",
        "        '--cuda', '0',\n",
        "        '--context_path', 'resnet18',\n",
        "        '--num_classes', '19'\n",
        "    ]\n",
        "    main(params)"
      ],
      "metadata": {
        "id": "kh7L1ZmEDHBv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "train.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}