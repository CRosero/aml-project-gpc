{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRosero/aml-project/blob/master/train_step_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the dataset from drive.\n",
        "( You can find the zipped folder [here](https://drive.google.com/file/d/1XsRmyQYHfgRFJCOueXpJ37yyOCrKHO-W/view?usp=sharing))"
      ],
      "metadata": {
        "id": "Y9D12pC0R_eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive/')\n",
        "data_path = \"/content/data\"\n",
        "\n",
        "use_complete_dataset = False\n",
        "\n",
        "if (use_complete_dataset == True) and (not os.path.isfile('/content/data.zip')):\n",
        "  print(\"download entire dataset\")\n",
        "  !gdown --id 1A2dBwPlCyXHTqmG1LRvPfVm6K21jWwUI # 3-5 min\n",
        "  !jar xf  \"/content/data.zip\"\n",
        "elif (use_complete_dataset == False) and (not os.path.isfile('/content/data.zip')):\n",
        "  # Load cropped dataset containing only 10 images\n",
        "  print(\"download cropped dataset\")\n",
        "  !gdown --id 1gPcwDJsNpyqcjKu225hnIxkURpjUA08i   \n",
        "  !jar xf  \"/content/data.zip\"\n",
        "else:\n",
        "  print(\"dataset already downloaded\")\n",
        "\n",
        "if not os.path.isdir('/content/data'):\n",
        "  print(\"Dataset doesn't exist\")"
      ],
      "metadata": {
        "id": "YOzT4IA9ZnnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9db2d9b-146e-4bf4-bb3f-6ea1f47a88b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n",
            "dataset already downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning the repository from github"
      ],
      "metadata": {
        "id": "e1nqqrU5SIBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "repo_path = \"/content/cloned-repo\"\n",
        "if not os.path.isdir(repo_path):\n",
        "  !git clone -l -s https://github.com/CRosero/aml-project.git cloned-repo\n",
        "  %cd cloned-repo\n",
        "else:\n",
        "  print(\"Repository already cloned\")\n",
        "%cd /content/cloned-repo\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uzIXW9Z5Kl4",
        "outputId": "4f677d98-b73c-4c18-a1f5-7380cf1a71b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already cloned\n",
            "/content/cloned-repo\n",
            "dataset     eval.py  model\t  README.md  train_step_2.ipynb  utils.py\n",
            "eval.ipynb  loss.py  __pycache__  train.py   train_step_3.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "_6lnZtpKPTHy",
        "outputId": "5005c3f5-246f-4562-8470-5f4b7c5b7c6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the libraries"
      ],
      "metadata": {
        "id": "A4np6uwUgDMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QYHTsZf6SkxK"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from model.build_BiSeNet import BiSeNet\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from loss import CrossEntropy2d\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from utils import poly_lr_scheduler\n",
        "from utils import reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu\n",
        "from loss import DiceLoss\n",
        "import torch.cuda.amp as amp\n",
        "import os\n",
        "import os.path as osp\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import torchvision\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "\n",
        "import json\n",
        "# Dataset class:\n",
        "from dataset.cityscapesDataSet_DA import cityscapesDataSet\n",
        "from dataset.GTA5DataSet import GTA5DataSet\n",
        "# Discriminator\n",
        "from model.discriminator import FCDiscriminator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "n9TWPNCay2Md",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e04805-4790-42bd-a03d-89baa7fbfb81"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_calc(pred, label, gpu):\n",
        "    \"\"\"\n",
        "    This function returns cross entropy loss for semantic segmentation\n",
        "    \"\"\"\n",
        "    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n",
        "    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n",
        "    label = Variable(label.long()).cuda(gpu)\n",
        "    criterion = CrossEntropy2d().cuda(gpu)\n",
        "\n",
        "    return criterion(pred, label)\n",
        "\n",
        "\n",
        "def lr_poly(base_lr, iter, max_iter, power):\n",
        "    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, i_iter):\n",
        "    lr = lr_poly(args.learning_rate, i_iter, args.num_steps, args.power)\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    if len(optimizer.param_groups) > 1:\n",
        "        optimizer.param_groups[1]['lr'] = lr * 10\n",
        "\n",
        "\n",
        "def adjust_learning_rate_D(optimizer, i_iter):\n",
        "    lr = lr_poly(args.learning_rate_D, i_iter, args.num_steps, args.power)\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    if len(optimizer.param_groups) > 1:\n",
        "        optimizer.param_groups[1]['lr'] = lr * 10\n"
      ],
      "metadata": {
        "id": "evfJ9HAhM9-a"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "\n",
        "MODEL = 'BiseNet'\n",
        "BATCH_SIZE = 3\n",
        "ITER_SIZE = 1\n",
        "NUM_WORKERS = 4\n",
        "DATA_DIRECTORY = '/content/data/'\n",
        "DATA_LIST_PATH = './train.txt'\n",
        "IGNORE_LABEL = 255\n",
        "INPUT_SIZE = '1280,720'\n",
        "DATA_DIRECTORY_TARGET = './data/Cityscapes'\n",
        "DATA_LIST_PATH_TARGET = './train.txt'\n",
        "INPUT_SIZE_TARGET = '1024,512'\n",
        "LEARNING_RATE = 2.5e-2\n",
        "MOMENTUM = 0.9\n",
        "NUM_CLASSES = 19\n",
        "NUM_STEPS = 250000\n",
        "NUM_STEPS_STOP = 150000  # early stopping\n",
        "POWER = 0.9\n",
        "RANDOM_SEED = 1234\n",
        "RESTORE_FROM =  '/gdrive/MyDrive/Project_AML/Models/checkpoints_segNet/'\n",
        "SAVE_NUM_IMAGES = 2\n",
        "SAVE_PRED_EVERY = 5000\n",
        "SNAPSHOT_DIR = '/gdrive/MyDrive/Project_AML/Models/snapshots_segNet/'\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "LEARNING_RATE_D = 1e-4\n",
        "LAMBDA_SEG = 0.1\n",
        "LAMBDA_ADV_TARGET1 = 0.0002\n",
        "LAMBDA_ADV_TARGET2 = 0.001\n",
        "GAN = 'Vanilla'\n",
        "\n",
        "TARGET = 'Cityscapes'\n",
        "SET = 'train'"
      ],
      "metadata": {
        "id": "KZnCIyrh3hfl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arguments(params=[]):\n",
        "    \"\"\"Parse all the arguments provided from the CLI.\n",
        "    Returns:\n",
        "      A list of parsed arguments.\n",
        "    \"\"\"\n",
        "        \n",
        "    # basic parameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=MODEL,\n",
        "                        help=\"available options : BiseNet\")\n",
        "    parser.add_argument(\"--target\", type=str, default=TARGET,\n",
        "                        help=\"available options : Cityscapes\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=BATCH_SIZE,\n",
        "                        help=\"Number of images sent to the network in one step.\")\n",
        "    parser.add_argument(\"--iter-size\", type=int, default=ITER_SIZE,\n",
        "                        help=\"Accumulate gradients for ITER_SIZE iterations.\")\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=NUM_WORKERS,\n",
        "                        help=\"number of workers for multithread dataloading.\")\n",
        "    parser.add_argument(\"--data-dir\", type=str, default=DATA_DIRECTORY,\n",
        "                        help=\"Path to the directory containing the source dataset.\")\n",
        "    parser.add_argument(\"--data-list\", type=str, default=DATA_LIST_PATH,\n",
        "                        help=\"Path to the file listing the images in the source dataset.\")\n",
        "    parser.add_argument(\"--ignore-label\", type=int, default=IGNORE_LABEL,\n",
        "                        help=\"The index of the label to ignore during the training.\")\n",
        "    parser.add_argument(\"--input-size\", type=str, default=INPUT_SIZE,\n",
        "                        help=\"Comma-separated string with height and width of source images.\")\n",
        "    parser.add_argument(\"--data-dir-target\", type=str, default=DATA_DIRECTORY_TARGET,\n",
        "                        help=\"Path to the directory containing the target dataset.\")\n",
        "    parser.add_argument(\"--data-list-target\", type=str, default=DATA_LIST_PATH_TARGET,\n",
        "                        help=\"Path to the file listing the images in the target dataset.\")\n",
        "    parser.add_argument(\"--input-size-target\", type=str, default=INPUT_SIZE_TARGET,\n",
        "                        help=\"Comma-separated string with height and width of target images.\")\n",
        "    parser.add_argument(\"--is-training\", action=\"store_true\",\n",
        "                        help=\"Whether to updates the running means and variances during the training.\")\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=LEARNING_RATE,\n",
        "                        help=\"Base learning rate for training with polynomial decay.\")\n",
        "    parser.add_argument(\"--learning-rate-D\", type=float, default=LEARNING_RATE_D,\n",
        "                        help=\"Base learning rate for discriminator.\")\n",
        "    parser.add_argument(\"--lambda-seg\", type=float, default=LAMBDA_SEG,\n",
        "                        help=\"lambda_seg.\")\n",
        "    parser.add_argument(\"--lambda-adv-target1\", type=float, default=LAMBDA_ADV_TARGET1,\n",
        "                        help=\"lambda_adv for adversarial training.\")\n",
        "    parser.add_argument(\"--lambda-adv-target2\", type=float, default=LAMBDA_ADV_TARGET2,\n",
        "                        help=\"lambda_adv for adversarial training.\")\n",
        "    parser.add_argument(\"--momentum\", type=float, default=MOMENTUM,\n",
        "                        help=\"Momentum component of the optimiser.\")\n",
        "    parser.add_argument(\"--not-restore-last\", action=\"store_true\",\n",
        "                        help=\"Whether to not restore last (FC) layers.\")\n",
        "    parser.add_argument(\"--num-classes\", type=int, default=NUM_CLASSES,\n",
        "                        help=\"Number of classes to predict (including background).\")\n",
        "    parser.add_argument(\"--num-steps\", type=int, default=NUM_STEPS,\n",
        "                        help=\"Number of training steps.\")\n",
        "    parser.add_argument(\"--num-steps-stop\", type=int, default=NUM_STEPS_STOP,\n",
        "                        help=\"Number of training steps for early stopping.\")\n",
        "    parser.add_argument(\"--power\", type=float, default=POWER,\n",
        "                        help=\"Decay parameter to compute the learning rate.\")\n",
        "    parser.add_argument(\"--random-mirror\", action=\"store_true\",\n",
        "                        help=\"Whether to randomly mirror the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-scale\", action=\"store_true\",\n",
        "                        help=\"Whether to randomly scale the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-seed\", type=int, default=RANDOM_SEED,\n",
        "                        help=\"Random seed to have reproducible results.\")\n",
        "    parser.add_argument(\"--restore-from\", type=str, default=RESTORE_FROM,\n",
        "                        help=\"Where restore model parameters from.\")\n",
        "    parser.add_argument(\"--save-num-images\", type=int, default=SAVE_NUM_IMAGES,\n",
        "                        help=\"How many images to save.\")\n",
        "    parser.add_argument(\"--save-pred-every\", type=int, default=SAVE_PRED_EVERY,\n",
        "                        help=\"Save summaries and checkpoint every often.\")\n",
        "    parser.add_argument(\"--snapshot-dir\", type=str, default=SNAPSHOT_DIR,\n",
        "                        help=\"Where to save snapshots of the model.\")\n",
        "    parser.add_argument(\"--weight-decay\", type=float, default=WEIGHT_DECAY,\n",
        "                        help=\"Regularisation parameter for L2-loss.\")\n",
        "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
        "                        help=\"choose gpu device.\")\n",
        "    parser.add_argument(\"--set\", type=str, default=SET,\n",
        "                        help=\"choose adaptation set.\")\n",
        "    parser.add_argument(\"--gan\", type=str, default=GAN,\n",
        "                        help=\"choose the GAN objective.\")\n",
        "    parser.add_argument('--context_path', type=str, default=\"resnet101\",\n",
        "                        help='The context path model you are using, resnet18, resnet101.')\n",
        "\n",
        "\n",
        "    args = parser.parse_args(params)\n",
        "    return args\n",
        "\n"
      ],
      "metadata": {
        "id": "S24rDhec9N0b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def main(params):\n",
        "\"\"\"Create the model and start the training.\"\"\"\n",
        "params = []\n",
        "args = get_arguments(params)\n",
        "\n",
        "w, h = map(int, args.input_size.split(','))\n",
        "input_size = (w, h)\n",
        "\n",
        "w, h = map(int, args.input_size_target.split(','))\n",
        "input_size_target = (w, h)\n",
        "\n",
        "cudnn.enabled = True\n",
        "gpu = args.gpu\n",
        "\n",
        "# Create network\n",
        "if args.model == 'BiseNet':\n",
        "  model = BiSeNet(num_classes=args.num_classes, context_path= args.context_path)\n",
        "  '''\n",
        "  if args.restore_from[:4] == 'http' :\n",
        "      saved_state_dict = model_zoo.load_url(args.restore_from)\n",
        "  else:\n",
        "      saved_state_dict = torch.load(args.restore_from)\n",
        "\n",
        "  new_params = model.state_dict().copy()\n",
        "  for i in saved_state_dict:\n",
        "      # Scale.layer5.conv2d_list.3.weight\n",
        "      i_parts = i.split('.')\n",
        "      # print i_parts\n",
        "      if not args.num_classes == 19 or not i_parts[1] == 'layer5':\n",
        "          new_params['.'.join(i_parts[1:])] = saved_state_dict[i]\n",
        "          print(i_parts)\n",
        "  model.load_state_dict(new_params)\n",
        "  '''\n",
        "# Set the network to train mode\n",
        "model.train()\n",
        "model.cuda(args.gpu)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# init D\n",
        "model_D1 = FCDiscriminator(num_classes=args.num_classes)\n",
        "model_D2 = FCDiscriminator(num_classes=args.num_classes)\n",
        "# Set the discriminators to train mode\n",
        "model_D1.train()\n",
        "model_D1.cuda(args.gpu)\n",
        "\n",
        "model_D2.train()\n",
        "model_D2.cuda(args.gpu)\n",
        "\n",
        "# check if the snapshot dir exists, if not create it\n",
        "if not os.path.exists(args.snapshot_dir):\n",
        "  os.makedirs(args.snapshot_dir)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "8X0hpXG5S0ta"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path\n",
        "source_data_root_path = os.path.join(args.data_dir, \"GTA5\") # /content/data/GTA5\n",
        "target_data_root_path = os.path.join(args.data_dir, args.target) # /content/data/Cityscapes\n",
        "source_train_path = os.path.join(source_data_root_path, \"train.txt\") # /content/data/GTA5/train.txt\n",
        "target_root_path = os.path.join(target_data_root_path,  \"train.txt\")   # /content/data/Cityscapes/train.txt\n",
        "info_path = os.path.join(source_data_root_path,  \"info.json\") # /content/data/GTA/info.json \n",
        "\n",
        "info_json = json.load(open(info_path))\n",
        "\n",
        "# Datasets  \n",
        "source_dataset = GTA5DataSet(source_data_root_path, source_train_path, info_json, crop_size=input_size, scale=args.random_scale, mirror=args.random_mirror, mean=IMG_MEAN)\n",
        "target_dataset = cityscapesDataSet(target_data_root_path, target_root_path, crop_size=input_size_target, scale=False, mirror=args.random_mirror, mean=IMG_MEAN)\n",
        "\n",
        "print(\"GTA: \", len(source_dataset))\n",
        "print(\"Cityscapes: \", len(target_dataset))\n",
        "img,label = source_dataset[0]\n",
        "print (\"GTA image\", img.shape )\n",
        "print (\"GTA label\", label.shape )\n",
        "img = target_dataset[0]\n",
        "print (\"Cityscapes image\", img.shape )\n",
        "\n",
        "# Create DataLoaders\n",
        "trainloader = data.DataLoader(source_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "trainloader_iter = enumerate(trainloader)\n",
        "\n",
        "targetloader = data.DataLoader(target_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "targetloader_iter = enumerate(targetloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "EPOjuXIBEpU9",
        "outputId": "f54e313e-9dd5-46d7-9ae3-88b40327f363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GTA:  10\n",
            "Cityscapes:  10\n",
            "GTA image (3, 720, 1280)\n",
            "GTA label (720, 1280)\n",
            "Cityscapes image (3, 512, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "\n",
        "# implement model.optim_parameters(args) to handle different models' lr setting\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "optimizer_D1 = optim.Adam(model_D1.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n",
        "optimizer_D1.zero_grad()\n",
        "\n",
        "optimizer_D2 = optim.Adam(model_D2.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n",
        "optimizer_D2.zero_grad()\n",
        "\n",
        "if args.gan == 'Vanilla':\n",
        "  bce_loss = torch.nn.BCEWithLogitsLoss()\n",
        "elif args.gan == 'LS':\n",
        "  bce_loss = torch.nn.MSELoss()\n",
        "\n",
        "interp = torch.nn.Upsample(size=(input_size[1], input_size[0]), mode='bilinear')\n",
        "interp_target = torch.nn.Upsample(size=(input_size_target[1], input_size_target[0]), mode='bilinear')\n",
        "\n",
        "# labels for adversarial training\n",
        "source_label = 0\n",
        "target_label = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "lS05jrSRI1u7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i_iter in range(args.num_steps):\n",
        "\n",
        "  loss_seg_value1 = 0\n",
        "  loss_adv_target_value1 = 0\n",
        "  loss_D_value1 = 0\n",
        "\n",
        "  loss_seg_value2 = 0\n",
        "  loss_adv_target_value2 = 0\n",
        "  loss_D_value2 = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  adjust_learning_rate(optimizer, i_iter)\n",
        "\n",
        "  optimizer_D1.zero_grad()\n",
        "  optimizer_D2.zero_grad()\n",
        "  adjust_learning_rate_D(optimizer_D1, i_iter)\n",
        "  adjust_learning_rate_D(optimizer_D2, i_iter)\n",
        "\n",
        "  for sub_i in range(args.iter_size):\n",
        "\n",
        "      # train G\n",
        "\n",
        "      # don't accumulate grads in D\n",
        "      for param in model_D1.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      for param in model_D2.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      # train with source\n",
        "\n",
        "      _, batch = next(trainloader_iter)\n",
        "      images, labels = batch\n",
        "      images = Variable(images).cuda(args.gpu)\n",
        "\n",
        "      result, pred1, pred2 = model(images) # TODO: ask\n",
        "      pred1 = interp(pred1)\n",
        "      pred2 = interp(pred2)\n",
        "\n",
        "      loss_seg1 = loss_calc(pred1, labels, args.gpu)\n",
        "      loss_seg2 = loss_calc(pred2, labels, args.gpu)\n",
        "      loss = loss_seg2 + args.lambda_seg * loss_seg1\n",
        "\n",
        "      # proper normalization\n",
        "      loss = loss / args.iter_size\n",
        "      loss.backward()\n",
        "      loss_seg_value1 += loss_seg1.data.cpu().numpy()[0] / args.iter_size\n",
        "      loss_seg_value2 += loss_seg2.data.cpu().numpy()[0] / args.iter_size\n",
        "\n",
        "      # train with target\n",
        "\n",
        "      _, batch = next(targetloader_iter)\n",
        "      images, _, _ = batch\n",
        "      images = Variable(images).cuda(args.gpu)\n",
        "\n",
        "      pred_target1, pred_target2 = model(images)\n",
        "      pred_target1 = interp_target(pred_target1)\n",
        "      pred_target2 = interp_target(pred_target2)\n",
        "\n",
        "      D_out1 = model_D1(F.softmax(pred_target1))\n",
        "      D_out2 = model_D2(F.softmax(pred_target2))\n",
        "\n",
        "      loss_adv_target1 = bce_loss(D_out1, Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(args.gpu))\n",
        "\n",
        "      loss_adv_target2 = bce_loss(D_out2, Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda( args.gpu))\n",
        "\n",
        "      loss = args.lambda_adv_target1 * loss_adv_target1 + args.lambda_adv_target2 * loss_adv_target2\n",
        "      loss = loss / args.iter_size\n",
        "      loss.backward()\n",
        "      loss_adv_target_value1 += loss_adv_target1.data.cpu().numpy()[0] / args.iter_size\n",
        "      loss_adv_target_value2 += loss_adv_target2.data.cpu().numpy()[0] / args.iter_size\n",
        "\n",
        "      # train D\n",
        "\n",
        "      # bring back requires_grad\n",
        "      for param in model_D1.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "      for param in model_D2.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "      # train with source\n",
        "      pred1 = pred1.detach()\n",
        "      pred2 = pred2.detach()\n",
        "\n",
        "      D_out1 = model_D1(F.softmax(pred1))\n",
        "      D_out2 = model_D2(F.softmax(pred2))\n",
        "\n",
        "      loss_D1 = bce_loss(D_out1, Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(args.gpu))\n",
        "\n",
        "      loss_D2 = bce_loss(D_out2, Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(args.gpu))\n",
        "\n",
        "      loss_D1 = loss_D1 / args.iter_size / 2\n",
        "      loss_D2 = loss_D2 / args.iter_size / 2\n",
        "\n",
        "      loss_D1.backward()\n",
        "      loss_D2.backward()\n",
        "\n",
        "      loss_D_value1 += loss_D1.data.cpu().numpy()[0]\n",
        "      loss_D_value2 += loss_D2.data.cpu().numpy()[0]\n",
        "\n",
        "      # train with target\n",
        "      pred_target1 = pred_target1.detach()\n",
        "      pred_target2 = pred_target2.detach()\n",
        "\n",
        "      D_out1 = model_D1(F.softmax(pred_target1))\n",
        "      D_out2 = model_D2(F.softmax(pred_target2))\n",
        "\n",
        "      loss_D1 = bce_loss(D_out1, Variable(torch.FloatTensor(D_out1.data.size()).fill_(target_label)).cuda(args.gpu))\n",
        "\n",
        "      loss_D2 = bce_loss(D_out2,Variable(torch.FloatTensor(D_out2.data.size()).fill_(target_label)).cuda(args.gpu))\n",
        "\n",
        "      loss_D1 = loss_D1 / args.iter_size / 2\n",
        "      loss_D2 = loss_D2 / args.iter_size / 2\n",
        "\n",
        "      loss_D1.backward()\n",
        "      loss_D2.backward()\n",
        "\n",
        "      loss_D_value1 += loss_D1.data.cpu().numpy()[0]\n",
        "      loss_D_value2 += loss_D2.data.cpu().numpy()[0]\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer_D1.step()\n",
        "  optimizer_D2.step()\n",
        "\n",
        "  print('exp = {}'.format(args.snapshot_dir))\n",
        "  print(\n",
        "  'iter = {0:8d}/{1:8d}, loss_seg1 = {2:.3f} loss_seg2 = {3:.3f} loss_adv1 = {4:.3f}, loss_adv2 = {5:.3f} loss_D1 = {6:.3f} loss_D2 = {7:.3f}'.format(\n",
        "      i_iter, args.num_steps, loss_seg_value1, loss_seg_value2, loss_adv_target_value1, loss_adv_target_value2, loss_D_value1, loss_D_value2))\n",
        "\n",
        "  if i_iter >= args.num_steps_stop - 1:\n",
        "      print ('save model ...')\n",
        "      torch.save(model.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(args.num_steps_stop) + '.pth'))\n",
        "      torch.save(model_D1.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(args.num_steps_stop) + '_D1.pth'))\n",
        "      torch.save(model_D2.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(args.num_steps_stop) + '_D2.pth'))\n",
        "      break\n",
        "\n",
        "  if i_iter % args.save_pred_every == 0 and i_iter != 0:\n",
        "      print ('taking snapshot ...')\n",
        "      torch.save(model.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(i_iter) + '.pth'))\n",
        "      torch.save(model_D1.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(i_iter) + '_D1.pth'))\n",
        "      torch.save(model_D2.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(i_iter) + '_D2.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "eaWWxSnE7_jB",
        "outputId": "4a7a5cf0-95d5-4039-e05c-8ba7ece37b9f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4d85ffc66332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0;31m# proper normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m       \u001b[0mloss_seg_value1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_seg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mloss_seg_value2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_seg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 370.00 MiB (GPU 0; 14.76 GiB total capacity; 12.16 GiB already allocated; 93.75 MiB free; 13.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "XbPSPiAYzYJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    params = [\n",
        "        \n",
        "    ]\n",
        "    main(params)"
      ],
      "metadata": {
        "id": "ryuEeQcMS4R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_summary()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "74xLh4HqXQT2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}