{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRosero/aml-project/blob/master/train_step_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the dataset from drive.\n",
        "( You can find the zipped folder [here](https://drive.google.com/file/d/1XsRmyQYHfgRFJCOueXpJ37yyOCrKHO-W/view?usp=sharing))"
      ],
      "metadata": {
        "id": "Y9D12pC0R_eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive/')\n",
        "data_path = \"/content/data\"\n",
        "\n",
        "use_complete_dataset = True\n",
        "\n",
        "if (use_complete_dataset == True) and (not os.path.isfile('/content/data.zip')):\n",
        "  print(\"download entire dataset\")\n",
        "  !gdown --id 1A2dBwPlCyXHTqmG1LRvPfVm6K21jWwUI # 3-5 min\n",
        "  !jar xf  \"/content/data.zip\"\n",
        "elif (use_complete_dataset == False) and (not os.path.isfile('/content/data.zip')):\n",
        "  # Load cropped dataset containing only 10 images\n",
        "  print(\"download cropped dataset\")\n",
        "  !gdown --id 1gPcwDJsNpyqcjKu225hnIxkURpjUA08i   \n",
        "  !jar xf  \"/content/data.zip\"\n",
        "else:\n",
        "  print(\"dataset already downloaded\")\n",
        "\n",
        "if not os.path.isdir('/content/data'):\n",
        "  print(\"Dataset doesn't exist\")"
      ],
      "metadata": {
        "id": "YOzT4IA9ZnnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa66776-1198-41eb-c38b-5d7f43549806"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n",
            "dataset already downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning the repository from github"
      ],
      "metadata": {
        "id": "e1nqqrU5SIBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "repo_path = \"/content/cloned-repo\"\n",
        "if not os.path.isdir(repo_path):\n",
        "  !git clone -l -s https://github.com/CRosero/aml-project.git cloned-repo\n",
        "  %cd cloned-repo\n",
        "else:\n",
        "  print(\"Repository already cloned\")\n",
        "%cd /content/cloned-repo\n",
        "!ls"
      ],
      "metadata": {
        "id": "3uzIXW9Z5Kl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4edd21-4d86-4e25-f75b-79905a37a835"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already cloned\n",
            "/content/cloned-repo\n",
            "dataset     image_output  __pycache__  train_step_2-experiment.ipynb  utils.py\n",
            "eval.ipynb  loss.py\t  README.md    train_step_2.ipynb\n",
            "eval.py     model\t  train.py     train_step_3.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "_6lnZtpKPTHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfae2d1-3444-4a72-f953-f3a8c9971df9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the libraries"
      ],
      "metadata": {
        "id": "A4np6uwUgDMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QYHTsZf6SkxK"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from model.build_BiSeNet import BiSeNet\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from loss import CrossEntropy2d\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from utils import poly_lr_scheduler\n",
        "from utils import reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu\n",
        "from loss import DiceLoss\n",
        "import torch.cuda.amp as amp\n",
        "import os\n",
        "import os.path as osp\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import torchvision\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import json\n",
        "# Dataset class:\n",
        "from dataset.cityscapesDataSet import cityscapesDataSet\n",
        "from dataset.GTA5DataSet import GTA5DataSet\n",
        "# Discriminator\n",
        "from model.discriminator import FCDiscriminator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def enable_cuda(obj, gpu):\n",
        "  if torch.cuda.is_available():\n",
        "    return obj.cuda(gpu)\n",
        "  else:\n",
        "    return obj"
      ],
      "metadata": {
        "id": "_S8mzM8TVfSC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "n9TWPNCay2Md",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9157bc0-ca34-40a1-d025-74937aaafa5a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_calc(pred, labels, gpu):\n",
        "    \"\"\"\n",
        "    This function returns cross entropy loss for semantic segmentation\n",
        "    \"\"\"\n",
        "    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n",
        "    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n",
        "    labels = Variable(labels.long()).cuda(gpu)\n",
        "    labels = enable_cuda(labels, gpu)\n",
        "    criterion = CrossEntropy2d()\n",
        "    criterion = enable_cuda(criterion, gpu)\n",
        "\n",
        "    return criterion(pred, labels)\n",
        "\n",
        "\n",
        "def lr_poly(base_lr, iter, max_iter, power):\n",
        "    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, i_iter):\n",
        "    lr = lr_poly(args.learning_rate, i_iter, args.num_steps, args.power)\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    if len(optimizer.param_groups) > 1:\n",
        "        optimizer.param_groups[1]['lr'] = lr * 10\n",
        "\n",
        "\n",
        "def adjust_learning_rate_D(optimizer, i_iter):\n",
        "    lr = lr_poly(args.learning_rate_D, i_iter, args.num_steps, args.power)\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    if len(optimizer.param_groups) > 1:\n",
        "        optimizer.param_groups[1]['lr'] = lr * 10\n"
      ],
      "metadata": {
        "id": "evfJ9HAhM9-a"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "\n",
        "MODEL = 'BiseNet'\n",
        "BATCH_SIZE = 2\n",
        "ITER_SIZE = 1\n",
        "NUM_WORKERS = 4\n",
        "DATA_DIRECTORY = '/content/data/'\n",
        "DATA_LIST_PATH = './train.txt'\n",
        "IGNORE_LABEL = 255\n",
        "INPUT_SIZE = '600,304'#'1280,720'\n",
        "DATA_DIRECTORY_TARGET = './data/Cityscapes'\n",
        "DATA_LIST_PATH_TARGET = './train.txt'\n",
        "INPUT_SIZE_TARGET = '600,304'#'1024,512'\n",
        "LEARNING_RATE = 2.5e-2\n",
        "MOMENTUM = 0.9\n",
        "NUM_CLASSES = 19\n",
        "NUM_STEPS = 500//BATCH_SIZE\n",
        "NUM_STEPS_STOP = 150\n",
        "POWER = 0.9\n",
        "RANDOM_SEED = 1234\n",
        "RESTORE_FROM =  '/gdrive/MyDrive/Project_AML/Models/checkpoints_segNet/'\n",
        "SAVE_NUM_IMAGES = 2\n",
        "SAVE_PRED_EVERY = 10\n",
        "SNAPSHOT_DIR = '/gdrive/MyDrive/Project_AML/Models/snapshots_segNet/'\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "LEARNING_RATE_D = 1e-4\n",
        "LAMBDA_SEG = 0.1\n",
        "LAMBDA_ADV_TARGET = 0.0002\n",
        "GAN = 'Vanilla'\n",
        "\n",
        "TARGET = 'Cityscapes'\n",
        "SET = 'train'"
      ],
      "metadata": {
        "id": "KZnCIyrh3hfl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arguments(params=[]):\n",
        "    \"\"\"Parse all the arguments provided from the CLI.\n",
        "    Returns:\n",
        "      A list of parsed arguments.\n",
        "    \"\"\"\n",
        "        \n",
        "    # basic parameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=MODEL,\n",
        "                        help=\"available options : BiseNet\")\n",
        "    parser.add_argument(\"--target\", type=str, default=TARGET,\n",
        "                        help=\"available options : Cityscapes\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=BATCH_SIZE,\n",
        "                        help=\"Number of images sent to the network in one step.\")\n",
        "    parser.add_argument(\"--iter-size\", type=int, default=ITER_SIZE,\n",
        "                        help=\"Accumulate gradients for ITER_SIZE iterations.\")\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=NUM_WORKERS,\n",
        "                        help=\"number of workers for multithread dataloading.\")\n",
        "    parser.add_argument(\"--data-dir\", type=str, default=DATA_DIRECTORY,\n",
        "                        help=\"Path to the directory containing the source dataset.\")\n",
        "    parser.add_argument(\"--data-list\", type=str, default=DATA_LIST_PATH,\n",
        "                        help=\"Path to the file listing the images in the source dataset.\")\n",
        "    parser.add_argument(\"--ignore-label\", type=int, default=IGNORE_LABEL,\n",
        "                        help=\"The index of the label to ignore during the training.\")\n",
        "    parser.add_argument(\"--input-size\", type=str, default=INPUT_SIZE,\n",
        "                        help=\"Comma-separated string with height and width of source images.\")\n",
        "    parser.add_argument(\"--data-dir-target\", type=str, default=DATA_DIRECTORY_TARGET,\n",
        "                        help=\"Path to the directory containing the target dataset.\")\n",
        "    parser.add_argument(\"--data-list-target\", type=str, default=DATA_LIST_PATH_TARGET,\n",
        "                        help=\"Path to the file listing the images in the target dataset.\")\n",
        "    parser.add_argument(\"--input-size-target\", type=str, default=INPUT_SIZE_TARGET,\n",
        "                        help=\"Comma-separated string with height and width of target images.\")\n",
        "    parser.add_argument(\"--is-training\", action=\"store_true\",\n",
        "                        help=\"Whether to updates the running means and variances during the training.\")\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=LEARNING_RATE,\n",
        "                        help=\"Base learning rate for training with polynomial decay.\")\n",
        "    parser.add_argument(\"--learning-rate-D\", type=float, default=LEARNING_RATE_D,\n",
        "                        help=\"Base learning rate for discriminator.\")\n",
        "    parser.add_argument(\"--lambda-seg\", type=float, default=LAMBDA_SEG,\n",
        "                        help=\"lambda_seg.\")\n",
        "    parser.add_argument(\"--lambda-adv-target\", type=float, default=LAMBDA_ADV_TARGET,\n",
        "                        help=\"lambda_adv for adversarial training.\")\n",
        "    parser.add_argument(\"--momentum\", type=float, default=MOMENTUM,\n",
        "                        help=\"Momentum component of the optimiser.\")\n",
        "    parser.add_argument(\"--not-restore-last\", action=\"store_true\",\n",
        "                        help=\"Whether to not restore last (FC) layers.\")\n",
        "    parser.add_argument(\"--num-classes\", type=int, default=NUM_CLASSES,\n",
        "                        help=\"Number of classes to predict (including background).\")\n",
        "    parser.add_argument(\"--num-steps\", type=int, default=NUM_STEPS,\n",
        "                        help=\"Number of training steps.\")\n",
        "    parser.add_argument(\"--num-steps-stop\", type=int, default=NUM_STEPS_STOP,\n",
        "                        help=\"Number of training steps for early stopping.\")\n",
        "    parser.add_argument(\"--power\", type=float, default=POWER,\n",
        "                        help=\"Decay parameter to compute the learning rate.\")\n",
        "    parser.add_argument(\"--random-mirror\", action=\"store_true\",\n",
        "                        help=\"Whether to randomly mirror the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-scale\", action=\"store_true\",\n",
        "                        help=\"Whether to randomly scale the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-seed\", type=int, default=RANDOM_SEED,\n",
        "                        help=\"Random seed to have reproducible results.\")\n",
        "    parser.add_argument(\"--restore-from\", type=str, default=RESTORE_FROM,\n",
        "                        help=\"Where restore model parameters from.\")\n",
        "    parser.add_argument(\"--save-num-images\", type=int, default=SAVE_NUM_IMAGES,\n",
        "                        help=\"How many images to save.\")\n",
        "    parser.add_argument(\"--save-pred-every\", type=int, default=SAVE_PRED_EVERY,\n",
        "                        help=\"Save summaries and checkpoint every often.\")\n",
        "    parser.add_argument(\"--snapshot-dir\", type=str, default=SNAPSHOT_DIR,\n",
        "                        help=\"Where to save snapshots of the model.\")\n",
        "    parser.add_argument(\"--weight-decay\", type=float, default=WEIGHT_DECAY,\n",
        "                        help=\"Regularisation parameter for L2-loss.\")\n",
        "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
        "                        help=\"choose gpu device.\")\n",
        "    parser.add_argument(\"--set\", type=str, default=SET,\n",
        "                        help=\"choose adaptation set.\")\n",
        "    parser.add_argument(\"--gan\", type=str, default=GAN,\n",
        "                        help=\"choose the GAN objective.\")\n",
        "    parser.add_argument('--context_path', type=str, default=\"resnet101\",\n",
        "                        help='The context path model you are using, resnet18, resnet101.')\n",
        "\n",
        "\n",
        "    args = parser.parse_args(params)\n",
        "    return args\n",
        "\n"
      ],
      "metadata": {
        "id": "S24rDhec9N0b"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def main(params):\n",
        "\"\"\"Create the model and start the training.\"\"\"\n",
        "params = []\n",
        "args = get_arguments(params)\n",
        "\n",
        "w, h = map(int, args.input_size.split(','))\n",
        "input_size = (w, h)\n",
        "\n",
        "w, h = map(int, args.input_size_target.split(','))\n",
        "input_size_target = (w, h)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "cudnn.enabled = True\n",
        "gpu = args.gpu\n",
        "\n",
        "# Create network\n",
        "if args.model == 'BiseNet':\n",
        "  model = BiSeNet(num_classes=args.num_classes, context_path= args.context_path)\n",
        "\n",
        "# Set the network to train mode\n",
        "model.train()\n",
        "model = enable_cuda(model, args.gpu)\n",
        "\n",
        "# init D\n",
        "model_D = FCDiscriminator(num_classes=args.num_classes)\n",
        "\n",
        "# Set the discriminators to train mode\n",
        "model_D.train()\n",
        "model_D = enable_cuda(model_D, args.gpu)\n",
        "\n",
        "# check if the snapshot dir exists, if not create it\n",
        "if not os.path.exists(args.snapshot_dir):\n",
        "  os.makedirs(args.snapshot_dir)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "8X0hpXG5S0ta"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path\n",
        "source_data_root_path = os.path.join(args.data_dir, \"GTA5\") # /content/data/GTA5\n",
        "target_data_root_path = os.path.join(args.data_dir, args.target) # /content/data/Cityscapes\n",
        "source_train_path = os.path.join(source_data_root_path, \"train.txt\") # /content/data/GTA5/train.txt\n",
        "target_root_path = os.path.join(target_data_root_path,  \"train.txt\")   # /content/data/Cityscapes/train.txt\n",
        "info_path = os.path.join(source_data_root_path,  \"info.json\") # /content/data/GTA/info.json \n",
        "\n",
        "info_json = json.load(open(info_path))\n",
        "\n",
        "# Datasets  \n",
        "source_dataset = GTA5DataSet(source_data_root_path, source_train_path, info_json, crop_size=input_size, scale=args.random_scale, mirror=args.random_mirror, mean=IMG_MEAN)\n",
        "target_dataset = cityscapesDataSet(target_data_root_path, target_root_path, info_json, crop_size=input_size_target, scale=False, mirror=args.random_mirror, mean=IMG_MEAN)\n",
        "\n",
        "print(\"GTA: \", len(source_dataset))\n",
        "print(\"Cityscapes: \", len(target_dataset))\n",
        "img,label = source_dataset[0]\n",
        "print (\"GTA image\", img.shape )\n",
        "print (\"GTA label\", label.shape )\n",
        "img, _ = target_dataset[0]\n",
        "print (\"Cityscapes image\", img.shape )\n",
        "\n",
        "# Create DataLoaders\n",
        "trainloader = data.DataLoader(source_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "trainloader_iter = enumerate(trainloader)\n",
        "\n",
        "targetloader = data.DataLoader(target_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "targetloader_iter = enumerate(targetloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "EPOjuXIBEpU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab3d1b2-8706-4d60-8d63-29c5934c545d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GTA:  10\n",
            "Cityscapes:  10\n",
            "GTA image (3, 304, 600)\n",
            "GTA label (304, 600)\n",
            "Cityscapes image (3, 304, 600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "\n",
        "# implement model.optim_parameters(args) to handle different models' lr setting\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "optimizer_D = optim.Adam(model_D.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n",
        "optimizer_D.zero_grad()\n",
        "\n",
        "if args.gan == 'Vanilla':\n",
        "  bce_loss = torch.nn.BCEWithLogitsLoss()\n",
        "elif args.gan == 'LS':\n",
        "  bce_loss = torch.nn.MSELoss()\n",
        "\n",
        "interp = torch.nn.Upsample(size=(input_size[1], input_size[0]), mode='bilinear')\n",
        "interp_target = torch.nn.Upsample(size=(input_size_target[1], input_size_target[0]), mode='bilinear')\n",
        "\n",
        "# labels for adversarial training\n",
        "source_label_id = 0\n",
        "target_label_id = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "lS05jrSRI1u7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i_iter in range(args.num_steps):\n",
        "\n",
        "  loss_seg_value = 0\n",
        "  loss_adv_target_value = 0\n",
        "  loss_D_value = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  adjust_learning_rate(optimizer, i_iter)\n",
        "\n",
        "  optimizer_D.zero_grad()\n",
        "  adjust_learning_rate_D(optimizer_D, i_iter)\n",
        "\n",
        "  for sub_i in range(args.iter_size):\n",
        "\n",
        "      # train G\n",
        "\n",
        "      # don't accumulate grads in D\n",
        "      for param in model_D.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      # train with source images and labels\n",
        "      _, batch = next(trainloader_iter)\n",
        "      source_images, source_labels = batch\n",
        "      source_images = Variable(source_images)\n",
        "      source_images = enable_cuda(source_images, args.gpu)\n",
        "\n",
        "      pred_source_result, pred_source_1, pred_source_2 = model(source_images)\n",
        "      loss1 = loss_calc(pred_source_result, source_labels, args.gpu)\n",
        "      loss2 = loss_calc(pred_source_1, source_labels, args.gpu)\n",
        "      loss3 = loss_calc(pred_source_2, source_labels, args.gpu)\n",
        "      loss_seg = loss1 + loss2 + loss3\n",
        "\n",
        "      # proper normalization\n",
        "      loss_seg = loss_seg / args.iter_size\n",
        "      loss_seg.backward()\n",
        "      print(loss_seg.data.cpu().numpy())\n",
        "      print(loss_seg.data.cpu().numpy().shape)\n",
        "      loss_seg_value += loss_seg.data.cpu().numpy() / args.iter_size\n",
        "\n",
        "      # train with target images\n",
        "      _, batch = next(targetloader_iter)\n",
        "      target_images, _ = batch\n",
        "      target_images = Variable(target_images).cuda(args.gpu)\n",
        "\n",
        "      pred_target_result, pred_target_1, pred_target_2 = model(target_images)\n",
        "\n",
        "      # generator vs. discriminator \n",
        "      D_out = model_D(F.softmax(pred_target_result))\n",
        "\n",
        "      loss_adv_target = bce_loss(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label_id)).cuda(args.gpu))\n",
        "      loss = args.lambda_adv_target * loss_adv_target\n",
        "      loss = loss / args.iter_size\n",
        "      loss.backward()\n",
        "      loss_adv_target_value += loss_adv_target.data.cpu().numpy() / args.iter_size\n",
        "\n",
        "      # train discriminator\n",
        "      # bring back requires_grad\n",
        "      for param in model_D.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "      # train with source\n",
        "      pred_source_result = pred_source_result.detach()\n",
        "      \n",
        "      D_out_source = model_D(F.softmax(pred_source_result))\n",
        "      loss_D_source = bce_loss(D_out_source, Variable(torch.FloatTensor(D_out_source.data.size()).fill_(source_label_id)).cuda(args.gpu))\n",
        "      loss_D_source = loss_D_source / args.iter_size / 2\n",
        "      loss_D_source.backward()\n",
        "\n",
        "      loss_D_value += loss_D_source.data.cpu().numpy()\n",
        "\n",
        "      # train with target\n",
        "      pred_target_result = pred_target_result.detach()\n",
        "\n",
        "      D_out_target = model_D(F.softmax(pred_target_result))\n",
        "\n",
        "      loss_D_target = bce_loss(D_out_target, Variable(torch.FloatTensor(D_out_target.data.size()).fill_(target_label_id)).cuda(args.gpu))\n",
        "      loss_D_target = loss_D_target / args.iter_size / 2\n",
        "      loss_D_target.backward()\n",
        "\n",
        "      loss_D_value += loss_D_target.data.cpu().numpy()\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer_D.step()\n",
        "\n",
        "  print('exp = {}'.format(args.snapshot_dir))\n",
        "  print('iter = {0:8d}/{1:8d}, loss_seg = {2:.3f}, loss_adv = {3:.3f}, loss_D = {4:.3f}'.format(i_iter, args.num_steps, loss_seg_value, loss_adv_target_value, loss_D_value))\n",
        "\n",
        "  if i_iter >= args.num_steps_stop - 1:\n",
        "      print ('save model ...')\n",
        "      torch.save(model.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(args.num_steps_stop) + '.pth'))\n",
        "      torch.save(model_D.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(args.num_steps_stop) + '_D.pth'))\n",
        "      break\n",
        "\n",
        "  if i_iter % args.save_pred_every == 0 and i_iter != 0:\n",
        "      print ('taking snapshot ...')\n",
        "      torch.save(model.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(i_iter) + '.pth'))\n",
        "      torch.save(model_D.state_dict(), osp.join(args.snapshot_dir, 'GTA5_' + str(i_iter) + '_D.pth'))"
      ],
      "metadata": {
        "id": "eaWWxSnE7_jB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "50ead1b9-2993-416c-dc63-2ca2768a1e9b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.274339\n",
            "()\n",
            "exp = /gdrive/MyDrive/Project_AML/Models/snapshots_segNet/\n",
            "iter =        0/     250, loss_seg = 10.274, loss_adv = 0.690, loss_D = 0.693\n",
            "9.11343\n",
            "()\n",
            "exp = /gdrive/MyDrive/Project_AML/Models/snapshots_segNet/\n",
            "iter =        1/     250, loss_seg = 9.113, loss_adv = 0.693, loss_D = 0.693\n",
            "8.176761\n",
            "()\n",
            "exp = /gdrive/MyDrive/Project_AML/Models/snapshots_segNet/\n",
            "iter =        2/     250, loss_seg = 8.177, loss_adv = 0.696, loss_D = 0.693\n",
            "7.028964\n",
            "()\n",
            "exp = /gdrive/MyDrive/Project_AML/Models/snapshots_segNet/\n",
            "iter =        3/     250, loss_seg = 7.029, loss_adv = 0.698, loss_D = 0.692\n",
            "7.4563913\n",
            "()\n",
            "exp = /gdrive/MyDrive/Project_AML/Models/snapshots_segNet/\n",
            "iter =        4/     250, loss_seg = 7.456, loss_adv = 0.699, loss_D = 0.692\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-cddf067ab3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;31m# train with source images and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0msource_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0msource_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0;31m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "b85FRez7ZxCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "XbPSPiAYzYJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    params = [\n",
        "        \n",
        "    ]\n",
        "    main(params)"
      ],
      "metadata": {
        "id": "ryuEeQcMS4R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_summary()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "74xLh4HqXQT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  '''\n",
        "  if args.restore_from[:4] == 'http' :\n",
        "      saved_state_dict = model_zoo.load_url(args.restore_from)\n",
        "  else:\n",
        "      saved_state_dict = torch.load(args.restore_from)\n",
        "\n",
        "  new_params = model.state_dict().copy()\n",
        "  for i in saved_state_dict:\n",
        "      # Scale.layer5.conv2d_list.3.weight\n",
        "      i_parts = i.split('.')\n",
        "      # print i_parts\n",
        "      if not args.num_classes == 19 or not i_parts[1] == 'layer5':\n",
        "          new_params['.'.join(i_parts[1:])] = saved_state_dict[i]\n",
        "          print(i_parts)\n",
        "  model.load_state_dict(new_params)\n",
        "  '''"
      ],
      "metadata": {
        "id": "E9nL-8fhfZQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}