{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRosero/aml-project/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import of the datasets from drive.\n",
        "\n",
        "*TODO: Importing in this way the data (from the drive) does not work if someone does not have the folder in his/her drive (for example the professor), we need therefore to find another way before sending the final version of the project! (just using the public link? or putting the data in a different git repository as in lab1?)*"
      ],
      "metadata": {
        "id": "Y9D12pC0R_eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive/')\n",
        "\n",
        "\n",
        "data_path = \"/content/data\"\n",
        "'''\n",
        "if not os.path.isdir(data_path):\n",
        "  !mkdir $data_path\n",
        "  !cp -r /gdrive/MyDrive/data/* $data_path\n",
        "else:\n",
        "  print(\"Data folder already existing\")\n",
        "'''\n",
        "if not os.path.isfile('/content/data.zip'):\n",
        "  !gdown --id 1XsRmyQYHfgRFJCOueXpJ37yyOCrKHO-W # 3-5 min\n",
        "  !jar xvf  \"/content/data.zip\"\n",
        "\n",
        "if not os.path.isdir('/content/data'):\n",
        "  print(\"Dataset doesn't exist\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YOzT4IA9ZnnX",
        "outputId": "c6431dbd-382a-4960-cad8-94c1e6d6e9a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -r /content/data"
      ],
      "metadata": {
        "id": "pBvDcOIsNzh_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning the repository from github"
      ],
      "metadata": {
        "id": "e1nqqrU5SIBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "repo_path = \"/content/cloned-repo\"\n",
        "if not os.path.isdir(repo_path):\n",
        "  !git clone -l -s git://github.com/CRosero/aml-project.git cloned-repo\n",
        "  %cd cloned-repo\n",
        "else:\n",
        "  print(\"Repository already cloned\")\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uzIXW9Z5Kl4",
        "outputId": "7179898a-e3c8-4d23-f33a-203389e9d102"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already cloned\n",
            "[Errno 2] No such file or directory: 'cloned-repo'\n",
            "/content/cloned-repo\n",
            "dataset     eval.py  model\t  README.md  train.ipynb  utils.py\n",
            "eval.ipynb  loss.py  __pycache__  runs\t     train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "A4np6uwUgDMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QYHTsZf6SkxK"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from model.build_BiSeNet import BiSeNet\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from utils import poly_lr_scheduler\n",
        "from utils import reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu\n",
        "from loss import DiceLoss\n",
        "import torch.cuda.amp as amp\n",
        "import os\n",
        "import os.path as osp\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import torchvision\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "n9TWPNCay2Md",
        "outputId": "1137b7fe-af5a-403f-bab7-7425a386cf29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset class:"
      ],
      "metadata": {
        "id": "yXC6NlGoSuZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_mapping(input, mapping):\n",
        "   '''\n",
        "   Given the input version of the labels (array of elements) performs a mapping using a mapping function and outputs the mapped labels\n",
        "   input = array of labels\n",
        "   mapping = array with format [oldlabel , newlabel]\n",
        "   output = array of mapped labels\n",
        "   '''\n",
        "   output = np.copy(input)\n",
        "   for ind in range(len(mapping)):\n",
        "     output[input == mapping[ind][0]] = mapping[ind][1]\n",
        "   return np.array(output, dtype=np.int64)"
      ],
      "metadata": {
        "id": "97sO91fxMs50"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cityscapesDataSet(data.Dataset):\n",
        "    def __init__(self, root, list_path, info_json, max_iters=None, crop_size=(321, 321), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255):\n",
        "        self.root = root\n",
        "        self.list_path = list_path\n",
        "        self.crop_size = crop_size\n",
        "        self.scale = scale\n",
        "        self.ignore_label = ignore_label\n",
        "        self.mean = mean\n",
        "        self.is_mirror = mirror\n",
        "        # in the list_path file of paths format [ name_of_folder/name_of_image ] -> img_ids list of paths format [name_of_image]\n",
        "        self.img_ids = [i_id.strip().split(\"/\")[1] for i_id in open(list_path)] \n",
        "        if not max_iters==None:\n",
        "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
        "        self.files = []\n",
        "\n",
        "        for name in self.img_ids:\n",
        "            img_file = osp.join(self.root , \"images\", name)\n",
        "            label_file = osp.join(self.root , \"images\", name)\n",
        "            self.files.append({\n",
        "                \"img\": img_file,\n",
        "                \"label\":label_file,\n",
        "                \"name\": name\n",
        "            })\n",
        "\n",
        "        self.mapping = np.array(info_json['label2train'], dtype=np.int)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        The __len__ method returns the length of the dataset\n",
        "        It is mandatory, as this is used by several other components\n",
        "        '''\n",
        "        return len(self.files)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        __getitem__ should access an element through its index\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (sample, target) where target is class_index of the target class.\n",
        "        '''\n",
        "        datafiles = self.files[index]\n",
        "        \n",
        "        image = Image.open(datafiles[\"img\"]).convert('RGB')\n",
        "        label = Image.open(datafiles[\"label\"])\n",
        "        name = datafiles[\"name\"]\n",
        "\n",
        "        # resize\n",
        "        image = image.resize(self.crop_size, Image.BILINEAR)\n",
        "        label = label.resize(self.crop_size, Image.NEAREST)\n",
        "        \n",
        "        # convert as array\n",
        "        image = np.asarray(image, np.float32)\n",
        "        label = np.asarray(label, np.float32)\n",
        "        \n",
        "        # map the labels\n",
        "        label = label_mapping(label, self.mapping)\n",
        "\n",
        "        # change to BGR\n",
        "        image = image[:, :, ::-1]  \n",
        "        # normalise\n",
        "        image -= self.mean\n",
        "        # transpose the image from HWC-layout (height, width, channels) -> (CHW layout)\n",
        "        image = image.transpose((2, 0, 1)) # see: https://github.com/isl-org/MiDaS/issues/79 \n",
        "\n",
        "        return image.copy(), label.copy()\n"
      ],
      "metadata": {
        "id": "_Iu4E8mdbpGu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(args, model, dataloader):\n",
        "    print('start val!')\n",
        "    # label_info = get_label_info(csv_path)\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        precision_record = []\n",
        "        hist = np.zeros((args.num_classes, args.num_classes))\n",
        "        for i, (data, label) in enumerate(dataloader):\n",
        "            label = label.type(torch.LongTensor)\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "\n",
        "            # get RGB predict image\n",
        "            predict = model(data).squeeze()\n",
        "            predict = reverse_one_hot(predict)\n",
        "            predict = np.array(predict.cpu())\n",
        "\n",
        "            # get RGB label image\n",
        "            label = label.squeeze()\n",
        "            if args.loss == 'dice':\n",
        "                label = reverse_one_hot(label)\n",
        "            label = np.array(label.cpu())\n",
        "\n",
        "            # compute per pixel accuracy\n",
        "            precision = compute_global_accuracy(predict, label)\n",
        "            hist += fast_hist(label.flatten(), predict.flatten(), args.num_classes)\n",
        "\n",
        "            # there is no need to transform the one-hot array to visual RGB array\n",
        "            # predict = colour_code_segmentation(np.array(predict), label_info)\n",
        "            # label = colour_code_segmentation(np.array(label), label_info)\n",
        "            precision_record.append(precision)\n",
        "        \n",
        "        precision = np.mean(precision_record)\n",
        "        # miou = np.mean(per_class_iu(hist))\n",
        "        miou_list = per_class_iu(hist)[:-1]\n",
        "        # miou_dict, miou = cal_miou(miou_list, csv_path)\n",
        "        miou = np.mean(miou_list)\n",
        "        print('precision per pixel for test: %.3f' % precision)\n",
        "        print('mIoU for validation: %.3f' % miou)\n",
        "        # miou_str = ''\n",
        "        # for key in miou_dict:\n",
        "        #     miou_str += '{}:{},\\n'.format(key, miou_dict[key])\n",
        "        # print('mIoU for each class:')\n",
        "        # print(miou_str)\n",
        "        return precision, miou"
      ],
      "metadata": {
        "id": "gnVsI55NSpcW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, model, optimizer, dataloader_train, dataloader_val):\n",
        "    writer = SummaryWriter(comment=''.format(args.optimizer, args.context_path))\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    if args.loss == 'dice':\n",
        "        loss_func = DiceLoss()\n",
        "    elif args.loss == 'crossentropy':\n",
        "        loss_func = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    \n",
        "    max_miou = 0\n",
        "    step = 0\n",
        "    \n",
        "    for epoch in range(args.num_epochs):\n",
        "        lr = poly_lr_scheduler(optimizer, args.learning_rate, iter=epoch, max_iter=args.num_epochs)\n",
        "        model.train()\n",
        "        tq = tqdm(total=len(dataloader_train) * args.batch_size)\n",
        "        tq.set_description('epoch %d, lr %f' % (epoch, lr))\n",
        "        loss_record = []\n",
        "        for i, (data, label) in enumerate(dataloader_train):\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with amp.autocast():\n",
        "                output, output_sup1, output_sup2 = model(data)\n",
        "                loss1 = loss_func(output, label)\n",
        "                loss2 = loss_func(output_sup1, label)\n",
        "                loss3 = loss_func(output_sup2, label)\n",
        "                loss = loss1 + loss2 + loss3\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "            tq.update(args.batch_size)\n",
        "            tq.set_postfix(loss='%.6f' % loss)\n",
        "            step += 1\n",
        "            writer.add_scalar('loss_step', loss, step)\n",
        "            loss_record.append(loss.item())\n",
        "            \n",
        "        tq.close()\n",
        "        loss_train_mean = np.mean(loss_record)\n",
        "        writer.add_scalar('epoch/loss_epoch_train', float(loss_train_mean), epoch)\n",
        "        print('loss for train : %f' % (loss_train_mean))\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            import os\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            torch.save(model.module.state_dict(),\n",
        "                       os.path.join(args.save_model_path, 'latest_dice_loss.pth'))\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != 0:\n",
        "            precision, miou = val(args, model, dataloader_val)\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os \n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                torch.save(model.module.state_dict(),\n",
        "                           os.path.join(args.save_model_path, 'best_dice_loss.pth'))\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)"
      ],
      "metadata": {
        "id": "quVpMD_PSwJS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(params):\n",
        "    # basic parameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--num_epochs', type=int, default=300, help='Number of epochs to train for')\n",
        "    parser.add_argument('--epoch_start_i', type=int, default=0, help='Start counting epochs from this number')\n",
        "    parser.add_argument('--checkpoint_step', type=int, default=10, help='How often to save checkpoints (epochs)')\n",
        "    parser.add_argument('--validation_step', type=int, default=10, help='How often to perform validation (epochs)')\n",
        "    parser.add_argument('--dataset', type=str, default=\"Cityscapes\", help='Dataset you are using.')\n",
        "    parser.add_argument('--crop_height', type=int, default=720, help='Height of cropped/resized input image to network')\n",
        "    parser.add_argument('--crop_width', type=int, default=960, help='Width of cropped/resized input image to network')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Number of images in each batch')\n",
        "    parser.add_argument(\"--iter-size\", type=int, default=1, help=\"Accumulate gradients for ITER_SIZE iterations.\")\n",
        "    parser.add_argument('--context_path', type=str, default=\"resnet101\",\n",
        "                        help='The context path model you are using, resnet18, resnet101.')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.01, help='learning rate used for train')\n",
        "    parser.add_argument('--data', type=str, default='content/data', help='path of training data')\n",
        "    #parser.add_argument(\"--data-list\", type=str, default='/gdrive/MyDrive/data/Cityscapes/train.txt', help=\"Path to the file listing the images in the source dataset.\")\n",
        "    #parser.add_argument(\"--data-list-val\", type=str, default='/gdrive/MyDrive/data/Cityscapes/val.txt', help=\"Path to the file listing the image in the test subset.\")\n",
        "    parser.add_argument('--num_workers', type=int, default=4, help='num of workers')\n",
        "    parser.add_argument('--num_classes', type=int, default=32, help='num of object classes (with void)')\n",
        "    parser.add_argument(\"--num-steps\", type=int, default=250000, help=\"Number of training steps.\")\n",
        "    parser.add_argument('--cuda', type=str, default='0', help='GPU ids used for training')\n",
        "    parser.add_argument('--use_gpu', type=bool, default=True, help='whether to user gpu for training')\n",
        "    parser.add_argument('--pretrained_model_path', type=str, default=None, help='path to pretrained model')\n",
        "    parser.add_argument('--save_model_path', type=str, default=None, help='path to save model')\n",
        "    parser.add_argument('--optimizer', type=str, default='rmsprop', help='optimizer, support rmsprop, sgd, adam')\n",
        "    parser.add_argument('--loss', type=str, default='crossentropy', help='loss function, dice or crossentropy')\n",
        "    parser.add_argument(\"--random-scale\", action=\"store_true\", help=\"Whether to randomly scale the inputs during the training.\")\n",
        "    parser.add_argument(\"--random-mirror\", action=\"store_true\", help=\"Whether to randomly mirror the inputs during the training.\")\n",
        "    parser.add_argument(\"--set-type\", type=str, default='train', help=\"choose adaptation set.\")\n",
        "\n",
        "    args = parser.parse_args(params)\n",
        "\n",
        "    # create dataset and dataloader\n",
        "    data_root_path = os.path.join(args.data, args.dataset) # /content/data/Cityscapes\n",
        "    train_path = os.path.join(data_root_path, \"train.txt\") # /content/data/Cityscapes/train.txt\n",
        "    val_path = os.path.join(data_root_path, \"val.txt\")   # /content/data/Cityscapes/val.txt\n",
        "    info_path = os.path.join(args.data, args.dataset, \"info.json\") # /content/data/Cityscapes/info.json \n",
        "    \n",
        "    # preprocessing informations:\n",
        "    input_size = (int(args.crop_height), int(args.crop_width))\n",
        "    f = open(info_path)\n",
        "    info = json.load(f)\n",
        "    img_mean = info[\"mean\"] # [73.15835921071155, 82.90891754262586, 72.39239876194159]\n",
        "    img_mean = np.array(img_mean, dtype=np.float32)\n",
        "    #img_mean = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "    #img_mean=(128, 128, 128)\n",
        "\n",
        "    \n",
        "    # create dataloaders\n",
        "    train_dataset = cityscapesDataSet(root=data_root_path,\n",
        "                                      list_path = train_path,\n",
        "                                      info_json = info,\n",
        "                                      crop_size=input_size,\n",
        "                                      scale=args.random_scale, \n",
        "                                      mirror=args.random_mirror, \n",
        "                                      mean=img_mean)\n",
        "   \n",
        "    \n",
        "    val_dataset = cityscapesDataSet(root=data_root_path,\n",
        "                                    list_path = val_path,\n",
        "                                    info_json = info,\n",
        "                                    crop_size=input_size,\n",
        "                                    scale=False,\n",
        "                                    mirror=args.random_mirror, \n",
        "                                    mean=img_mean)\n",
        "    \n",
        "    print(f'train_dataset: {len(train_dataset)}')\n",
        "    print(f'val_dataset: {len(val_dataset)}')\n",
        "    \n",
        "    # Define dataloaders\n",
        "    dataloader_train = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "    \n",
        "    dataloader_val = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
        "    \n",
        "    # build model\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.cuda\n",
        "    \n",
        "    model = BiSeNet(args.num_classes, args.context_path)\n",
        "    \n",
        "    if torch.cuda.is_available() and args.use_gpu:\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    # build optimizer\n",
        "    if args.optimizer == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), args.learning_rate)\n",
        "    elif args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), args.learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), args.learning_rate)\n",
        "    else:  # rmsprop\n",
        "        print('not supported optimizer \\n')\n",
        "        return None\n",
        "\n",
        "    # load pretrained model if exists\n",
        "    if args.pretrained_model_path is not None:\n",
        "        print('load model from %s ...' % args.pretrained_model_path)\n",
        "        model.module.load_state_dict(torch.load(args.pretrained_model_path))\n",
        "        print('Done!')\n",
        "\n",
        "    # train\n",
        "    train(args, model, optimizer, dataloader_train, dataloader_val)\n",
        "\n",
        "    # val(args, model, dataloader_val, csv_path)\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "8X0hpXG5S0ta"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "XbPSPiAYzYJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    params = [\n",
        "        '--num_epochs', '50',\n",
        "        '--learning_rate', '2.5e-2',\n",
        "        '--data', data_path,\n",
        "        '--num_workers', '4',\n",
        "        '--num_classes', '19',\n",
        "        '--cuda', '0',\n",
        "        '--batch_size', '5',\n",
        "        '--save_model_path', '/gdrive/MyDrive/Project_AML/Models/checkpoints_101_adam',\n",
        "        '--context_path', 'resnet101',  # set resnet18 or resnet101, only support resnet18 and resnet101\n",
        "        '--optimizer', 'adam',\n",
        "\n",
        "    ]\n",
        "    main(params)"
      ],
      "metadata": {
        "id": "ryuEeQcMS4R5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}